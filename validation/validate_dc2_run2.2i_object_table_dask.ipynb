{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection of DC2 Run 2.2i DR6 Object Table with Dask\n",
    "### Michael Wood-Vasey (@wmwv)\n",
    "### Last Verified to Run: 2020-07-14 by MWV\n",
    "\n",
    "Inspect the Run 2.2i DR6 Object Table  \n",
    "Using a Dask Cluster on one NERSC node as the backend.\n",
    "\n",
    "#### Run 2.2i DR6a as of 2020-06-04 includes  \n",
    "  * 78 tracts\n",
    "  * 52 million objects  \n",
    "  * 34 million objects with i-band SNR > 5\n",
    "\n",
    "Logistics:\n",
    "\n",
    "1. These tests were conducted on NERSC through the https://jupyter.nersc.gov interface.  \n",
    "Note: To enable re-rastering when zooming, use the JupyterLab Classic interface.\n",
    "You can launch this from an active JupyterHub Notebook by selecting \"Help->Launch Classic Notebook\".\n",
    "  * You can select the \"Running\" tab and then select the Notebook you want.\n",
    "  * You could instead browse through the full filesystem path under the \"Files\" tab to find your Notebook, but that's a lot more clicking.  You may want to take this aproach to launch some other Notebook that's not currently running under JupyterHub.\n",
    "\n",
    "2. Requires:\n",
    "```\n",
    "dask\n",
    "dask.distributed\n",
    "holoviews\n",
    "datashader\n",
    "bokeh\n",
    "pyarrow >= 0.13.1\n",
    "```\n",
    "\n",
    "Up-to-date versions of each of these are available in `desc-python-bleed` kernel\n",
    "\n",
    "3. This was run using the `desc-python-bleed` kernel\n",
    "\n",
    "We directly use the DPDD Parquet files.\n",
    "\n",
    "4. We use Dask, HoloViews, and Datashader to read Parquet files.  For more on each of these, see:\n",
    "\n",
    "References:  \n",
    "    https://dask.org  \n",
    "    https://datashader.org  \n",
    "    https://parquet.apache.org  \n",
    "    https://holoviews.org  \n",
    "    \n",
    "In brief:\n",
    "\n",
    "### Parquet\n",
    "\n",
    "Parquet is a column-based storage format that's part of a wider Arrow project to provide standardized, high-performance data representations in memory and on disk.  It's commonly used in current data science and large data volume processing, and is the current selected standard for Rubin Observatory LSST Data Management on-disk representations of output data catalogs.  The DESC Data Access Team is thus similarly using Parquet as the default underlying data format for representations of DC2 data as processed by the LSST DM Science Pipelines.\n",
    "\n",
    "### Dask\n",
    "\n",
    "Dask allows us to do processing by dividing tasks into individual workers.  These workers allow us to take fuller use of available memory and processors, including those on other machines.\n",
    "\n",
    "Dask is solving the needs to:\n",
    "\n",
    "1. Load more data than fit into memory. You can delay this in either time or space.\n",
    "   * Delaying in time would be if you running on a memory-limited machine, then Dask will be able to chunk through the work units without simultaneously needing the full amount of memory to hold all of the data at once.\n",
    "   * Delaying in space means spinning up additional machines.  This is often particularly powerful when connecting your front-end machine (e.g., a NERSC JupyterHub job is limited to 42 GB memory), to several full cluster compute nodes (e.g., a NERSC Haswell node is 32 real cores, 128 GB).  \n",
    "\n",
    "2. Distributing work across multiple processors. Python and numpy/scipy are not naturally parallel or easily parallelizable. One of the common things we will do with large datasets is aggregate for both analysis and visualization. Being able to do this aggregation in parallel is a significant gain.\n",
    "\n",
    "\n",
    "### HoloViews\n",
    "\n",
    "HoloViews is Dask aware and can provide Dask the correct information to build a Task Graph that effectively parallelizes the requisite data loading and computation.  HoloViews can use either bokeh or matplotlib backends.  If you directly use the matplotlib backend with a Dask DataFrame it will not appropriately parallelize across the workers and instead do lots of stuff in serial.  Bokeh also gives some nice interactive capabilities and HoloViews knows how to appropriate set up the linking and call backs to enable coordinated zooming and selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "1. I can't figure how to get Histograms to not have vertical lines.\n",
    "    * It may not be possible because I think they're being drawn with Bokeh `quad`.\n",
    "    * When I try to plot with `logy=True`, the plots become blank.\n",
    "    * Yes, it really does alphabetize the samples.  I would like to learn how to force the order so I get can `galaxy` to appear more clearly on top of `good`.\n",
    "    * If I instead use Path plot elements instead of Histogram, I can't figure out how to get labeled legends.\n",
    "2. Plot colorbars for data-shaded plots\n",
    "3. Histograms don't display scaled `logy`.  See, https://github.com/holoviz/holoviews/issues/2591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import astropy.units as u\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "from holoviews.operation import histogram\n",
    "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize\n",
    "from holoviews.plotting.util import process_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = 'viridis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start our Dask Cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple testing and illustration of how to use dask, holoview, and datashader here you can run locally on just one tract.\n",
    "To run on the full set of DR6, you'll need to set up a node to support Dask distributed.  Basically you just need a machine that can hold the data in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a local Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DASK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if LOCAL_DASK:\n",
    "    client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Dask Cluster on an Interactive Nodes\n",
    "\n",
    "So instead, in a separate Terminal on Cori, ask for a pair of Node from the `interactive` queue.  This generally completes in seconds.  We ask for 2 Nodes because we'd like the full ~256 GB of availablel memory to store the data and the intermediate copies that often get made in some of the plots below:\n",
    "\n",
    "Move to somewhere CSCRATCH directory to simplify file locking.  We're going to use `CSCRATCH` because that will be consistent across the nodes, whereas `SCRATCH` sometimes is empty on the compute nodes.\n",
    "```\n",
    "cd $CSCRATCH/\n",
    "```\n",
    "\n",
    "```\n",
    "salloc -N 2 -C haswell --qos=interactive -t 04:00:00\n",
    "```\n",
    "\n",
    "And then once on the first Node, where you'll get put after the `salloc` complets, load the right Python environment:\n",
    "```\n",
    "python /global/common/software/lsst/common/miniconda/start-kernel-cli.py desc-python-bleed\n",
    "```\n",
    "\n",
    "And then start up the Dask Cluster\n",
    "```\n",
    "NUM_WORKERS=16\n",
    "SCHEDULER_FILE=${CSCRATCH}/scheduler.json\n",
    "dask-scheduler --scheduler-file ${SCHEDULER_FILE} &\n",
    "dask-worker --nprocs ${NUM_WORKERS} --scheduler-file ${SCHEDULER_FILE} &\n",
    "```\n",
    "\n",
    "Then exit the environment and go to the second Node.\n",
    "\n",
    "```\n",
    "cd $CSCRATCH\n",
    "```\n",
    "\n",
    "```\n",
    "python /global/common/software/lsst/common/miniconda/start-kernel-cli.py desc-python-bleed\n",
    "```\n",
    "\n",
    "```\n",
    "NUM_WORKERS=16\n",
    "SCHEDULER_FILE=${CSCRATCH}/scheduler.json\n",
    "dask-worker --nprocs ${NUM_WORKERS} --scheduler-file ${SCHEDULER_FILE} &\n",
    "```\n",
    "\n",
    "The nodes will be printed out when the `salloc` launches.  And if you forget, you can look them up under the `SLURM_NODELIST` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connect to this Dask cluster through a shared agreement on where the `SCHEDULER_FILE` is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure the dashboard URL to use the JupyterHub proxy service.\n",
    "We here set the formatting string template to the correct value.\n",
    "Once we actually connect the client, then client can then tell us the full link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOCAL_DASK:\n",
    "    scheduler_file = os.path.join(os.environ[\"SCRATCH\"], \"scheduler.json\")\n",
    "    dask.config.config[\"distributed\"][\"dashboard\"][\"link\"] = \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{host}:{port}/status\"\n",
    "    client = Client(scheduler_file=scheduler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Catalog and Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_FILE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data_dir = f\"/global/cfs/cdirs/lsst/shared/DC2-prod/\"\n",
    "# Local copy on my own machine for testing and for when NERSC is down\n",
    "if LOCAL_FILE:\n",
    "    desc_data_dir = f\"/Users/wmwv/tmp/DC2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_release = \"dr6a\"\n",
    "\n",
    "run_data_dir = f\"Run2.2i/dpdd/Run2.2i-{data_release}/dc2_object_run2.2i_{data_release}\"\n",
    "datafile = os.path.join(desc_data_dir, run_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ('u', 'g', 'r', 'i', 'z', 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ra', 'dec']\n",
    "columns += [f'mag_{f}' for f in filters]\n",
    "columns += [f'magerr_{f}' for f in filters]\n",
    "columns += [f'mag_{f}_cModel' for f in filters]\n",
    "columns += [f'magerr_{f}_cModel' for f in filters]\n",
    "columns += [f'I_flag']\n",
    "columns += [f'I_flag_{f}' for f in filters]\n",
    "columns += [f'Ixx_{f}' for f in filters]\n",
    "columns += [f'Ixy_{f}' for f in filters]\n",
    "columns += [f'Iyy_{f}' for f in filters]\n",
    "columns += [f'psf_fwhm_{f}' for f in filters]\n",
    "columns += ['good', 'extendedness', 'blendedness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select good detections:\n",
    "#  1. Marked as 'good' in catalog flags.\n",
    "#  2. SNR in given band > threshold\n",
    "#  3. In defined simulation range\n",
    "snr_threshold = 5\n",
    "snr_filter = 'i'\n",
    "\n",
    "# We want to do a SNR cut, but magerr is the thing already calculated\n",
    "# So we'll redefine our SNR in terms of magerr\n",
    "magerr_cut = (2.5 / np.log(10)) / snr_threshold\n",
    "snr_cut = f'magerr_{snr_filter} < {magerr_cut}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(datafile, columns=columns, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color columns\n",
    "df['u-g'] = df['mag_u'] - df['mag_g']\n",
    "df['g-r'] = df['mag_g'] - df['mag_r']\n",
    "df['r-i'] = df['mag_r'] - df['mag_i']\n",
    "df['i-z'] = df['mag_i'] - df['mag_z']\n",
    "df['z-y'] = df['mag_z'] - df['mag_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.isfinite('blendedness')\n",
    "\n",
    "good = df[df[\"good\"] & (df[f\"magerr_{snr_filter}\"] < magerr_cut)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star = good[good['extendedness'] == 0]\n",
    "galaxy = good[good['extendedness'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total: {len(df)}, Good: {len(good)}, Stars: {len(star)}, Galaxies: {len(galaxy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the data in Dask Cluster Worker memory\n",
    "\n",
    "Dask actively purges data from memory when its no longer needed by the Dask Task Graph currently doing the computation.\n",
    "\n",
    "But each plot below is its own separate computation.  Dask doesn't know that it's going to use those data again in the next plot.  So we explicitly tell Dask to persist this data frame.\n",
    "\n",
    "If you don't have the physical memory across your Dask installation (whether local or remote), then skip this persist step.  Running each of the plots will require re-reading the data and bit a bit slower than if we had memory to keep all of the data, but will work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = good.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Density in RA, Dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DC2 Run 2.x WFD and DDF regions\n",
    "https://docs.google.com/document/d/18nNVImxGioQ3tcLFMRr67G_jpOzCIOdar9bjqChueQg/view\n",
    "https://github.com/LSSTDESC/DC2_visitList/blob/master/DC2visitGen/notebooks/DC2_Run2_regionCoords_WFD.ipynb\n",
    "\n",
    "| Location          | RA (degrees) | Dec (degrees) | RA (degrees) | Dec (degrees) |\n",
    "|:----------------- |:------------ |:------------- |:------------ |:------------- |\n",
    "| Region            | WFD          | WFD           | DDF          | DDF           |\n",
    "| Center            | 61.856114    | -35.79        | 53.125       | -28.100       |\n",
    "| North-East Corner | 71.462228    | -27.25        | 53.764       | -27.533       |\n",
    "| North-West Corner | 52.250000    | -27.25        | 52.486       | -27.533       |\n",
    "| South-West Corner | 49.917517    | -44.33        | 52.479       | -28.667       |\n",
    "| South-East Corner | 73.794710    | -44.33        | 53.771       | -28.667       |\n",
    "\n",
    "(Note that the order of the rows above is different than in the DC2 papers.  The order of the rows above goes around the perimeter in order.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc2_run2x_wfd = [[71.462228, -27.25], [52.250000, -27.25], [49.917517, -44.33], [73.794710, -44.33]]\n",
    "dc2_run2x_ddf = [[53.764, -27.533], [52.486, -27.533], [52.479, -28.667], [53.771, -28.667]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc2_run2x_wfd_df = pd.DataFrame({'ra': [coord[0] for coord in dc2_run2x_wfd] + [dc2_run2x_wfd[0][0]],\n",
    "                                 'dec': [coord[1] for coord in dc2_run2x_wfd] + [dc2_run2x_wfd[0][1]]})\n",
    "dc2_run2x_ddf_df = pd.DataFrame({'ra': [coord[0] for coord in dc2_run2x_ddf] + [dc2_run2x_ddf[0][0]],\n",
    "                                 'dec': [coord[1] for coord in dc2_run2x_ddf] + [dc2_run2x_ddf[0][1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ra_dec(df, dc2_run2x_wfd_df=dc2_run2x_wfd_df, dc2_run2x_ddf_df=dc2_run2x_ddf_df,\n",
    "                show_dc2_region=True, cmap=\"bmy\", bins=100, cmin=10):\n",
    "    \"\"\"We're just doing this on a rectilinear grid.\n",
    "    We should do a projection, of course, but that distortion is tolerable in this space.\"\"\"\n",
    "    points_ra_dec = hv.Points(df, kdims=[hv.Dimension('ra', soft_range=(dc2_run2x_wfd[2][0], dc2_run2x_wfd[3][0])),\n",
    "                                         hv.Dimension('dec', soft_range=(dc2_run2x_wfd[3][1], dc2_run2x_wfd[1][1]))])\n",
    "    # We have to define the colormap here now, because the opts aren't passed through the datashade->Points.\n",
    "    # See, e.g., https://github.com/holoviz/holoviews/issues/4125\n",
    "    ra_dec = datashade(points_ra_dec, cmap=process_cmap(cmap, provider=\"colorcet\"))\n",
    "    ra_dec = ra_dec.opts(invert_xaxis=True)  # Flip to East left\n",
    "    \n",
    "    if show_dc2_region:\n",
    "        # This region isn't quite a polygon.  The sides should be curved.\n",
    "        wfd_region = hv.Path(dc2_run2x_wfd_df).opts(color='red')\n",
    "        ddf_region = hv.Path(dc2_run2x_ddf_df).opts(color='orange')\n",
    "        ra_dec = ra_dec * wfd_region * ddf_region\n",
    "        \n",
    "        max_delta_ra = dc2_run2x_wfd_df['ra'][3] - dc2_run2x_wfd_df['ra'][2]\n",
    "        delta_dec = dc2_run2x_wfd_df['dec'][1] - dc2_run2x_wfd_df['dec'][3]\n",
    "        grow_buffer = 0.05\n",
    "\n",
    "        # Notice that these are specified in increasing RA left->right\n",
    "        # We rely on the invert_xaxis True above to flip this in the display\n",
    "        # It's important to get this right because these ranges are used for data selection\n",
    "        # and then the range is flipped in the display.\n",
    "        ra_dec.opts(xlim=(dc2_run2x_wfd[2][0] - max_delta_ra * grow_buffer,\n",
    "                    dc2_run2x_wfd[3][0] + max_delta_ra * grow_buffer))\n",
    "        ra_dec.opts(ylim=(dc2_run2x_wfd[3][1] - delta_dec * grow_buffer,\n",
    "                    dc2_run2x_wfd[1][1] + delta_dec * grow_buffer))\n",
    "\n",
    "    \n",
    "    return ra_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ra_dec(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    1. Make aspect ratio square.\n",
    "    2. Think about spherical->2D projection issues in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall object density distribution looks good.\n",
    "\n",
    "Notes:\n",
    "* If you are viewing this through a direct JupyterLab connection (Jupyter Classic Notebook, or separately on your own machine or setup), the plot will re-raster as you zoom in and out.  This functionality is not available within the JupyterHub environment.  JupyterHub doesn't allow the JavaScript callbacks in the browser back to the server that are necessary to do the re-rastering.\n",
    "* We explicitly excluded the tracts that overlap the DDF region (orange square upper-right corner).\n",
    "* There are also a few patches that failed within the main region.\n",
    "* There is an overall gradient N/S in object density, because we're plotting in rectilinear RA, Dec bins, which means that bins at the bottom in RA cover less area than those at the top.\n",
    "\n",
    "See the input visit coverage map here:\n",
    "https://github.com/LSSTDESC/ImageProcessingPipelines/issues/97#issuecomment-498303504\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color-Color Diagrams and the Stellar Locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We refer to a file over in `tutorials/assets' for the stellar locus\n",
    "datafile_davenport = '../tutorials/assets/Davenport_2014_MNRAS_440_3430_table1.txt'\n",
    "\n",
    "def get_stellar_locus_davenport(color1='gmr', color2='rmi',\n",
    "                                datafile=datafile_davenport):\n",
    "    color1 = color1.replace('-', 'm')\n",
    "    color2 = color2.replace('-', 'm')\n",
    "\n",
    "    data = pd.read_table(datafile, sep='\\s+', header=1)\n",
    "    return data[color1], data[color2]\n",
    "\n",
    "    \n",
    "def plot_stellar_locus(color1='gmr', color2='rmi',\n",
    "                       color='blue', line_dash='dashed', line_width=2.5,\n",
    "                       ax=None):\n",
    "\n",
    "    color1_m = color1.replace('-', 'm')\n",
    "    color2_m = color2.replace('-', 'm')\n",
    "\n",
    "    model_color1, model_color2 = get_stellar_locus_davenport(color1_m, color2_m)\n",
    "    model_df = pd.DataFrame({color1: model_color1, color2: model_color2})\n",
    "    stellar_locus = hv.Path(model_df).opts(color='blue', line_dash=line_dash, line_width=line_width)\n",
    "        \n",
    "    return stellar_locus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_color_color(df, color1, color2, \n",
    "                     range1=(-1, +2), range2=(-1, +2),\n",
    "                     cmin=10, cmap='rainbow',\n",
    "                     vmin=None, vmax=None):\n",
    "    \"\"\"Plot a color-color diagram.  Overlay stellar locus\"\"\"\n",
    "    band1, band2 = color1[0], color1[-1]\n",
    "    band3, band4 = color2[0], color2[-1]\n",
    "\n",
    "    clean = df[np.isfinite(df[color1]) & np.isfinite(df[color2])]\n",
    "    points_color1_color2 = hv.Points(\n",
    "        clean,\n",
    "        kdims=[\n",
    "            hv.Dimension(color1, range=range1),\n",
    "            hv.Dimension(color2, range=range2)]\n",
    "    )\n",
    "\n",
    "    color1_color2 = datashade(points_color1_color2, cmap=process_cmap(cmap, provider='colorcet'))\n",
    "\n",
    "    try:\n",
    "        stellar_locus = plot_stellar_locus(color1, color2)\n",
    "        color1_color2 = color1_color2 * stellar_locus\n",
    "    except KeyError as e:\n",
    "        print(f\"Couldn't plot Stellar Locus model for {color1}, {color2}\")\n",
    "        \n",
    "    return color1_color2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_four_color_color(df, vmin=0, vmax=50000):\n",
    "    layout = hv.Layout(\n",
    "    plot_color_color(df, 'g-r', 'u-g') + \\\n",
    "    plot_color_color(df, 'g-r', 'r-i') + \\\n",
    "    plot_color_color(df, 'g-r', 'i-z') + \\\n",
    "    plot_color_color(df, 'g-r', 'z-y'))\n",
    "    \n",
    "    layout = layout.cols(2)\n",
    "    \n",
    "    return layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above panels will zoom in `g-r` together because HoloViews knows that they share this data column.  They don't zoom \"together\" in the y-axes because those columns are not shared between the plots.\n",
    "\n",
    "The plots each re-raster as you zoom in and out.\n",
    "\n",
    "There is no brushing (selection) and linking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_four_color_color(star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discrete islands in the data for stellar color-color plot -- most visible in `r-i` vs. `g-r` at g-r ~= 1.2 mag -- are due to the finite set of stellar models used for simulating M dwarfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Let's plot the galaxies on the same color-color plots\n",
    "\n",
    "Clearly one doesn't expect the galaxies to follow the stellar locus.  But including the stellar locus lines makes it easy to guide the eye between the stars-only and the galaxies-only plots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_four_color_color(galaxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions for further study:\n",
    "   1. Is there a better comparison sample for the stellar locus than the Davenport reference?\n",
    "   2. Why is the stellar locus in the Davenport 0.1--0.2 mag redder for the reddest stars than the observed data.  Are there different extinction assumptions (this should be a low-extinction region).  Are there different bandpasses used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Density Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare number densities, we have to calculate the area covered by each catalog.\n",
    "We'll use Healpix through HealPy to pixelate the region and then count of the number of pixels with significant numbers of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(df, threshold=0.25, nside=1024, verbose=False):\n",
    "    \"\"\"Calculate the area covered by a catalog with 'ra', 'dec'\n",
    "    \n",
    "    Parameters:\n",
    "    --\n",
    "    cat: DataFrame, dict-like with 'ra', 'dec', keys\n",
    "    threshold:  float\n",
    "        Fraction of median value required to count a pixel.\n",
    "    nside:  int\n",
    "        Healpix NSIDE.  NSIDE=1024 is ~12 sq arcmin/pixel, NSIDE=4096 is 0.74 sq. arcmin/pixel\n",
    "        Increasing nside will decrease calculated area as holes become better resolved \n",
    "        and relative Poisson fluctuations in number counts become more significant.\n",
    "    verbose:  bool\n",
    "        Print details on nside, number of significant pixels, and area/pixel.\n",
    "        \n",
    "    Returns:\n",
    "    --\n",
    "    area:  Astropy Quantity.\n",
    "    \"\"\"\n",
    "    import healpy as hp\n",
    "\n",
    "    indices = hp.ang2pix(nside, df['ra'], df['dec'], lonlat=True)\n",
    "    idx, counts = np.unique(indices, return_counts=True)\n",
    "    \n",
    "    # Take the 25% of the median value of the non-zero counts/pixel\n",
    "    threshold_counts = threshold * np.median(counts)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Median {np.median(counts)} objects/pixel')\n",
    "        print(f'Only count pixels with more than {threshold_counts} objects')\n",
    "\n",
    "    significant_pixels, = np.where(counts > threshold_counts)\n",
    "    area_pixel = hp.nside2pixarea(nside, degrees=True) * u.deg**2\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Pixel size ~ {hp.nside2resol(nside, arcmin=True) * u.arcmin:0.2g}')\n",
    "        print(f'nside: {nside}, area/pixel: {area_pixel:0.4g}, num significant pixels: {len(significant_pixels)}')\n",
    "\n",
    "    area = len(significant_pixels) * area_pixel\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Total area: {area:0.7g}')\n",
    "    \n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dc2 = calculate_area(galaxy)\n",
    "print(f'DC2 Run 2.2i area: {area_dc2:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_den_dc2 = len(galaxy) / area_dc2\n",
    "\n",
    "# Change default expression to 1/arcmin**2\n",
    "num_den_dc2 = num_den_dc2.to(1/u.arcmin**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mag_density_hist(df, filt, bins=None, density=False, area=None, color=None, **kwargs):\n",
    "    mag_col = f'mag_{filt}'\n",
    "    frequencies, edges = np.histogram(df[mag_col], bins=bins, density=density)\n",
    "    if area is not None:\n",
    "        frequencies = frequencies / area.value\n",
    "    hist = hv.Histogram((edges, frequencies))\n",
    "    hist.opts(xlabel=filt)\n",
    "    \n",
    "    if area is not None:\n",
    "        ylabel = f\"Objects/{area.unit}/bin\"\n",
    "    else:\n",
    "        ylabel = \"Objects/bin\"\n",
    "    hist.opts(ylabel=ylabel)\n",
    "\n",
    "    if color:\n",
    "        hist.opts(fill_color=None)\n",
    "        hist.opts(line_color=color)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def plot_mag_density_path(df, filt, object_type='', bins=None, density=False, area=None, **kwargs):\n",
    "    mag_col = f'mag_{filt}'\n",
    "    frequencies, edges = np.histogram(df[mag_col], bins=bins, density=density)\n",
    "    if area is not None:\n",
    "        frequencies = frequencies / area\n",
    "    path = hv.Path(pd.DataFrame({mag_col: (edges[:-1]+edges[1:])/2, 'frequencies': frequencies}),\n",
    "                  label=object_type)\n",
    "#    if object_type is not None:\n",
    "#        path.opts(label=object_type)\n",
    "\n",
    "    path.opts(xlabel=mag_col)\n",
    "    \n",
    "    if area is not None:\n",
    "        ylabel = f\"Objects/{area.unit}/bin\"\n",
    "    else:\n",
    "        ylabel = \"Objects/bin\"\n",
    "    path.opts(ylabel=ylabel)\n",
    "    \n",
    "    return path \n",
    "\n",
    "plot_mag_density = plot_mag_density_hist\n",
    "\n",
    "def plot_mag_densities(good, star, galaxy, filt,\n",
    "                       area=None,\n",
    "                       log=False, range=(16, 32), bins=None,\n",
    "                       legend_position='top_left'):\n",
    "    if bins is None:\n",
    "        bins = np.linspace(*range, 100)\n",
    "    \n",
    "    densities = {'good': plot_mag_density(df, filt, object_type='good', bins=bins, area=area, color='green'),\n",
    "                 'star': plot_mag_density(star, filt, object_type='star', bins=bins, area=area, color='blue'),\n",
    "                 'galaxy': plot_mag_density(galaxy, filt, object_type='galaxy', bins=bins, area=area, color='red')}\n",
    "\n",
    "    overlay = hv.NdOverlay(densities, kdims='Sample')\n",
    "    overlay.opts(show_legend=True, legend_position=legend_position)\n",
    "    if log:\n",
    "        overlay.opts(logy=True)\n",
    "    \n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_plots = [plot_mag_densities(good, star, galaxy, filt, area=area_dc2) for filt in filters]\n",
    "density_plots[0].opts(legend_position='top_left')\n",
    "layout = hv.Layout(density_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sharp cut in i-band is because that was the reference band for most detections.  The distributions in the other bands extend to 28th mag because many of the forced-photometry measurements are consistent with 0 and our S/N cut above was on i-band flux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude Error vs. Magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude uncertainties come directly from the poisson estimates of the flux measurements.  By construction they will follow smooth curves.  We here confirm that they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mag_magerr(df, band, ax=None, range=(16, 28), magerr_limit=0.25, vmin=100,\n",
    "                   cmap=\"rainbow\", snr_magerr_threshold=magerr_cut):\n",
    "    mag_col, magerr_col = f'mag_{band}', f'magerr_{band}'\n",
    "    points_mag_magerr = hv.Points(df, kdims=[hv.Dimension(mag_col, range=(14, 28)),\n",
    "                                             hv.Dimension(magerr_col, range=(0, snr_magerr_threshold))])\n",
    "    return datashade(points_mag_magerr, cmap=process_cmap(cmap, provider='colorcet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_magerr = hv.Layout([plot_mag_magerr(good, filt) for filt in filters])\n",
    "mag_magerr.cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blendedness\n",
    "\n",
    "Blendedness is a measure of how much the identified flux from an object is affected by overlapping from other objects.\n",
    "\n",
    "See Bosch et al., 2018, Section 4.9.11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{100 * len(w)/len(good_idx):0.1f}% of objects have finite blendedness measurements.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question for futher study:  What happened to yield non-finite blendedness measurements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blendedness = datashade(hv.Points(good, kdims=['mag_i', 'blendedness']), cmap=process_cmap(\"rainbow\", provider=\"colorcet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blendedness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extendedness\n",
    " \n",
    "Extendedness is essentially star/galaxy separation based purely on morphology in the main detected reference band (which is `i` for most Objects).\n",
    "\n",
    "Extendedness a binary property in the catalog, so it's either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extendedness = datashade(hv.Points(good, kdims=['mag_i', 'extendedness']), cmap=process_cmap(\"rainbow\", provider=\"colorcet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extendedness.opts(ylim=(-0.1, +1.1)) * hv.Text(18, 0.9, \"Galaxies\") * hv.Text(18, 0.1, \"Stars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the first plot above made extendedness look like a simple binary property, the truth is more complicated.\n",
    "\n",
    "As galaxies get smaller in angular size and lower in signal-to-noise ratio, it becomes harder to clearly distinguish stars from galaxies.\n",
    "\n",
    "Extendedness is based off of the difference between the point-source model and extended model brightness.  Specifically objects with `mag_psf - mag_cmodel > 0.164` mag are labeled with `extendedness=1` (i.e., galaxies).\n",
    "\n",
    "See Bosch et al. 2018, Section 4.9.10 for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extendedness_delta_mag_cut = 0.0164\n",
    "psf_cModel_mag_cut = hv.VLine(extendedness_delta_mag_cut,\n",
    "                              label=rf\"{extendedness_delta_mag_cut:0.4f} $\\Delta$mag cut\")\n",
    "psf_cModel_mag_cut = psf_cModel_mag_cut.opts(color='red', line_dash=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_mag_cModel(df, filt, bins=None):\n",
    "    if bins is None:\n",
    "        bins = np.linspace(-0.1, 0.1, 201)\n",
    "    frequencies, edges = np.histogram(df[f'mag_{filt}'] - good[f'mag_{filt}_cModel'], bins=bins)\n",
    "    return hv.Histogram((edges, frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = 'i'\n",
    "delta_mag_cModel_hists = {'good': plot_delta_mag_cModel(good, filt),\n",
    "                          'star': plot_delta_mag_cModel(star, filt),\n",
    "                          'galaxy': plot_delta_mag_cModel(galaxy, filt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mag_cModel = hv.NdOverlay(delta_mag_cModel_hists, kdims=\"Sample\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mag_cModel.opts(width=600, xlabel='mag_i[_psf] - mag_i_CModel', ylabel='Objects/bin') \\\n",
    "  * psf_cModel_mag_cut \\\n",
    "  * hv.Text(-0.05, 4000, \"Stars\") * hv.Text(0.05, 4000, \"Galaxies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good['delta_mag_cModel_i'] = good['mag_i'] - good['mag_i_cModel']\n",
    "clean = good[(-2.5 < good['g-r']) & (good['g-r'] < 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_cModel_mag_cut = hv.HLine(extendedness_delta_mag_cut,\n",
    "                              label=rf\"{extendedness_delta_mag_cut:0.4f} $\\Delta$mag cut\")\n",
    "psf_cModel_mag_cut = psf_cModel_mag_cut.opts(color='green', line_dash=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = hv.Points(clean, kdims=['mag_i', 'delta_mag_cModel_i'])\n",
    "points = points.opts(xlabel='mag_i[_psf] - mag_cModel_i')\n",
    "\n",
    "yhist = points.hist(dimension='delta_mag_cModel_i', adjoin=False)\n",
    "xhist = points.hist(dimension='mag_i', adjoin=False)\n",
    "\n",
    "shaded_points = datashade(points, cmap=process_cmap(\"rainbow\", provider=\"colorcet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_color = hv.Points(clean, kdims=['g-r', 'delta_mag_cModel_i'])\n",
    "points_color_xhist = points_color.hist(dimension='g-r', dynamic=True, adjoin=False)\n",
    "\n",
    "shaded_points_color = datashade(points_color, cmap=process_cmap(\"rainbow\", provider=\"colorcet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = (shaded_points_color << yhist << points_color_xhist) \\\n",
    "    + (shaded_points * psf_cModel_mag_cut << yhist << xhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can zoom in a little to see how the fixed 0.0164 mag cut works at the low SNR limit.  Specifically at mag 24, we're starting to run out of stars and most things are galaxies.  But that's a population prior, it's not something visible using just morphology information.\n",
    "\n",
    "You can see the effect of lower SNR measurements as the horizontal line at $\\Delta$mag=0 puff up due to increased uncertainties.\n",
    "\n",
    "TODO: \n",
    "1. I don't know how to construct an AdjointLayout without a \"right\" element.  So there's an extra duplicate \"delta_mag_cModel_i\" histogram that's not really helpful or projected right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape Parameters\n",
    "\n",
    "Ixx, Iyy, Ixy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_hist(data, bins, color, line_dash):\n",
    "    frequencies, edges = np.histogram(data, bins=bins)\n",
    "    hist = hv.Histogram((edges, frequencies))\n",
    "    hist.opts(fill_color=color)\n",
    "    hist.opts(line_dash=line_dash)\n",
    "    return hist\n",
    " \n",
    "\n",
    "def plot_moments_for_filter(good, star, galaxy, filt,\n",
    "                            names=['good', 'star', 'galaxy'],\n",
    "                            colors=['blue', 'orange', 'green']):\n",
    "    hist_kwargs = {'color': colors, 'log': True,\n",
    "             'range': (0, 50)}\n",
    "\n",
    "    bins = np.logspace(-1, 1.5, 100)\n",
    "    moment_lines = {}\n",
    "    for prefix, ls in (('Ixx', 'solid'), ('Iyy', 'dashed'), ('Ixy', 'dotted')):\n",
    "        field = f'{prefix}_{filt}'\n",
    "        for df, name, color in zip((good, star, galaxy), names, colors):\n",
    "            label = f'{prefix} {name}'\n",
    "            line = plot_data_hist(good[field], bins=bins, color=color, line_dash=ls)\n",
    "            moment_lines[label] = line\n",
    "\n",
    "    moments_plot = hv.NdOverlay(moment_lines, kdims=\"Moments\")\n",
    "    moments_plot.opts(xlabel=f'{filt} Moments: Ixx, Iyy, Ixy [pixels^2]')\n",
    "    moments_plot.opts(ylabel='objects / bin')\n",
    "    \n",
    "    return moments_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment_plots = [plot_moments_for_filter(good, star, galaxy, filt) for filt in filters]\n",
    "for m in moment_plots[1:]:\n",
    "    m.opts(show_legend=False)\n",
    "moments = hv.Layout(moment_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments.cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "1. Need to clean up histograms so that you can see the different lines.\n",
    "2. Shift to a legend that's outside the grid of plots\n",
    "\n",
    "The stars (orange) are concentrated at low values of the source moments.\n",
    "\n",
    "Would be interesting to\n",
    "1. Look by magnitude or SNR to undersatnd the longer tail.  Are these galaxies mis-classified as stars, or are these noise sources?\n",
    "2. Distribution of ellipticity (see validate_drp to type this right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipticity_col_does_not_work(I_xx, I_xy, I_yy):\n",
    "    \"\"\"Calculate ellipticity from second moments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    I_xx : float or numpy.array\n",
    "    I_xy : float or numpy.array\n",
    "    I_yy : float or numpy.array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    e, e1, e2 : (float, float, float) or (numpy.array, numpy.array, numpy.array)\n",
    "        Complex ellipticity, real component, imaginary component\n",
    "        \n",
    "    Copied from https://github.com/lsst/validate_drp/python/lsst/validate/drp/util.py\n",
    "    \"\"\"\n",
    "    e = (I_xx - I_yy + 2j*I_xy) / (I_xx + I_yy + 2*dask.array.sqrt(I_xx*I_yy - I_xy**2))\n",
    "    e1 = np.real(e)\n",
    "    e2 = np.imag(e)\n",
    "    return e, e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipticity(df, Ixx, Ixy, Iyy):\n",
    "    \"\"\"Calculate ellipticity from second moments from a dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    Ixx : column name\n",
    "    Ixy : column name\n",
    "    Iyy : column name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    e, e1, e2 : (float, float, float) or (numpy.array, numpy.array, numpy.array)\n",
    "        Complex ellipticity, real component, imaginary component\n",
    "        \n",
    "    Copied from https://github.com/lsst/validate_drp/python/lsst/validate/drp/util.py\n",
    "    \"\"\"\n",
    "    e =  (df[Ixx] - df[Iyy] + 2j*df[Ixy] ) / (df[Ixx] + df[Iyy] + 2*dask.array.sqrt(df[Ixx]*df[Iyy] - df[Ixy]**2))\n",
    "    e1 = np.real(e)\n",
    "    e2 = np.imag(e)\n",
    "    return e, e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ellipticities_for_filter(good, star, galaxy, filt,\n",
    "                                names=['good', 'star', 'galaxy'],\n",
    "                                colors=['blue', 'orange', 'green']):\n",
    "    hist_kwargs = {'color': colors, 'log': True, 'range': (0, 50)}\n",
    "\n",
    "    bins = np.linspace(0, 20, 201)\n",
    "    ellipticity_lines = {}\n",
    "    for df, name, color in zip((good, star, galaxy), names, colors):\n",
    "        e, e1, e2 = \\\n",
    "        ellipticity(df, f'Ixx_{filt}', f'Ixy_{filt}', f'Iyy_{filt}')\n",
    "        for data, prefix, ls in ((e, 'e', 'solid'), (e1, 'e1', 'dashed'), (e2, 'e2', 'dotted')):\n",
    "            field = f'{prefix}_{filt}'\n",
    "            label = f'{prefix} {name}'\n",
    "            line = plot_data_hist(data, bins=bins, color=color, line_dash=ls)\n",
    "            ellipticity_lines[label] = line\n",
    "\n",
    "    ellipticities_plot = hv.NdOverlay(ellipticity_lines, kdims=\"Ellipticities\")\n",
    "    ellipticities_plot.opts(xlabel=f'{filt} Ellipticity: e, e1, e2 [pixels^2]')\n",
    "    ellipticities_plot.opts(ylabel='objects / bin')\n",
    "\n",
    "    return ellipticities_plot    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ell = plot_ellipticities_for_filter(good, star, galaxy, filt='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ellipticity_plots = [plot_ellipticities_for_filter(good, star, galaxy, filt) for filt in filters]\n",
    "for m in ellipticity_plots[1:]:\n",
    "    m.opts(show_legend=False)\n",
    "    \n",
    "# logy=True results in a blank plot.  This is a bug in holoviews+bokeh\n",
    "# for m in ellipticity_plots:\n",
    "#     m.opts(logy=True)\n",
    "#     m.opts(ylim=(10, 100000))\n",
    "\n",
    "ellipticities = hv.Layout(ellipticity_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipticities.cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    1. logy results in nothing being shown on plots\n",
    "    2. Full set of ellipticity calculations+plotting takes ~30 seconds on 8 workers on 16 core, 64 GB machine.  Should I pre-compute this earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWHM of the PSF\n",
    "At the location of the catalog objects.\n",
    "\n",
    "The Object Table stores the shape parameters of the PSF model as evaluated at the location of the object.\n",
    "\n",
    "This is not the same as, but is certainly related to, the distribution of effective seeing in the individual images that made up the coadd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_psf_fwhm(df, filt, bins=None, density=True):\n",
    "    psf_col = f\"psf_fwhm_{filt}\"\n",
    "    frequencies, edges = np.histogram(df[psf_col], density=density, bins=bins)\n",
    "    hist = hv.Histogram((edges, frequencies))\n",
    "    hist.opts(xlabel=psf_col)\n",
    "    return hist\n",
    "\n",
    "def plot_psf_fwhm_for_filters(df, filters=filters, bins=None, density=True,\n",
    "                              alpha=0.5,\n",
    "                              colors=(\"purple\", \"blue\", \"green\", \"orange\", \"red\", \"brown\")):\n",
    "    if bins is None:\n",
    "        bins = np.linspace(0, 1.5, 201)\n",
    "\n",
    "    fwhm_histograms = {}\n",
    "    for filt, color in zip(filters, colors):\n",
    "        hist = plot_psf_fwhm(df, filt, bins=bins, density=density)\n",
    "        hist.opts(color=color)\n",
    "        hist.opts(line_color=None)\n",
    "        hist.opts(line_alpha=alpha)\n",
    "        fwhm_histograms[filt] = hist\n",
    "    \n",
    "    fwhm = hv.NdOverlay(fwhm_histograms, kdims=\"Filter\")\n",
    "    if density:\n",
    "        ylabel = \"Density [Normalize to sum=1]\"\n",
    "    else:\n",
    "        ylabel = \"Objects / bin\"\n",
    "    fwhm.opts(xlabel=\"Model PSF FWHM [arcsec]\")\n",
    "    fwhm.opts(ylabel=ylabel)\n",
    "    \n",
    "    return fwhm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_psf_fwhm_for_filters(df).opts(width=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
