{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing DC2 data in PostgreSQL at NERSC part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Owner: **Joanne Bogart [@jrbogart](https://github.com/LSSTDESC/DC2-analysis/issues/new?body=@jrbogart)**  \n",
    "Last Verified to Run: \n",
    "\n",
    "This notebook introduces some additional features of the PostgreSQL database at NERSC.   \n",
    "\n",
    "__Learning objectives__:\n",
    "\n",
    "After going through this notebook, you should be able to:\n",
    "  1. Discover which object catalogs are available\n",
    "  2. Query on native quantities in those catalogs\n",
    "  3. Make use of custom functions, in particular for area searches\n",
    "\n",
    "__Logistics__: This notebook is intended to be run through the JupyterHub NERSC interface available here: https://jupyter-dev.nersc.gov. To setup your NERSC environment, please follow the instructions available here: https://confluence.slac.stanford.edu/display/LSSTDESC/Using+Jupyter-dev+at+NERSC\n",
    "\n",
    "### Prerequisites\n",
    "Please see the first notebook in this series for instructions on how to gain access to the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the db connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = 'desc_dc2_drp'\n",
    "dbuser = 'desc_dc2_drp_user'\n",
    "dbhost = 'nerscdb03.nersc.gov'\n",
    "dbconfig = {'dbname' : dbname, 'user' : dbuser, 'host' : dbhost}\n",
    "dbconn = psycopg2.connect(**dbconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Data\n",
    "Tables for the Run1.2i data as well as a view to make dpdd quantities more easily accessible are in the `schema` (acts like a namespace) `run12i`.  To reference, say, a table called `position` for Run1.2i use `run12i.position`. \n",
    "\n",
    "### Finding Datasets\n",
    "To find out which datasets are available and by what schema names, query the table `run_provenance`. It's in a special schema known as `public` which does not normally need to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['schema_name', 'run_designation','simulation_program', 'db_ingest', 'remarks']\n",
    "hdrs = ['schema_name', 'run_desig', 'sim_prog','db_ingest', 'remarks']\n",
    "# Additional columns in run_provenance store software and input data versions\n",
    "prov_query = 'SELECT '  + ','.join(cols) + ' from run_provenance'\n",
    "with dbconn.cursor() as cursor:\n",
    "    cursor.execute(prov_query)\n",
    "    fmt = '{0!s:14} {1!s:10} {2!s:9} {3!s:15} {4!s}'\n",
    "    print(fmt.format(hdrs[0], hdrs[1], hdrs[2], hdrs[3], hdrs[4]))\n",
    "    for record in cursor:\n",
    "        print(fmt.format(record[0], record[1], record[2], record[3], record[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally only datasets where the `db_ingest` field contains 'complete' are of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of the supported datasets\n",
    "schema = 'run12p_v4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying on Native Quantities\n",
    "Unlike DPDD quantities (all in a single view), native quantities are split across several tables. The first notebook in the PostgreSQL collection shows how to find out which tables belong to a schema and what columns a table has. Alternatively, if you know the column names you want, you can query for the table name.  The following looks for the table containing `ext_shapeHSM_HsmShapeRegauss_resolution`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'ext_shapehsm_hsmshaperegauss_flag'\n",
    "find_table_query = \"select table_name from information_schema.columns where table_schema='{}' and column_name='{}'\"\n",
    "find_table_query = find_table_query.format(schema, column)\n",
    "print(find_table_query)\n",
    "with dbconn.cursor() as cursor:\n",
    "    cursor.execute(find_table_query)\n",
    "    for record in cursor:\n",
    "        print(record[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some necessary fussiness here:\n",
    "* Note `ext_shapeHSM_HsmShapeRegauss_flag` has been transformed to all lower-case in the query. This is required when querying information_schema, where this string is a __value__ in the database (not a column name). \n",
    "* In the query single quotes are used around literals like `run12p_v4`. Double quotes won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we wanted to combine a cut on this quantity, in table dpdd_ref, with cuts on DPDD quantities like `clean`.  Then the query has to be made on a join of these two tables, where we specify that the value of dpdd_ref.object_id = dpdd.objectid. This causes the corresponding rows from each table (or view) to be treated as if they were assembled into one long row. Here is a simple query showing how this is done. A more realistic one would have more conditions in the `where` clause and might join more than two tables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'run12i'\n",
    "join_query = 'select count(object_id) from {schema}.dpdd join {schema}.dpdd_ref '\n",
    "join_query += 'on {schema}.dpdd_ref.object_id = {schema}.dpdd.objectid'\n",
    "join_query = join_query.format(**locals())\n",
    "where = \" where (ext_shapeHSM_HSmShapeRegauss_flag = 'f') and clean\"\n",
    "join_query += where\n",
    "print(join_query)                # confirm the query looks reasonable\n",
    "with dbconn.cursor() as cursor:\n",
    "    %time cursor.execute(join_query)\n",
    "    for record in cursor:\n",
    "        print(record[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjustments for Larger Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above query I switched to a different, significantly smaller dataset (2.8 million objects rather than 13.7 million).  For the much larger 2.1i_dr1b_v1 (over 78 million objects) we have to pay some attention to performance.\n",
    "The dpdd view is formed from a join of 3 tables. **dpdd_ref** is one of them, so in the above query that table is joined to itself, increasing the resources needed to make the query unnecessarily.  It's more efficient to just get all quantities from a join of tables only, but the dpdd view is more than just the result of a join with some of the columns renamed.  `clean` is formed by doing logical operations on several native-quantity flags. Starting with run 2.1i_dr1b_v1 it can be expressed as ```good and not deblend_skipped``` where both `good` and `deblend_skipped` are columns in **dpdd_ref**. (In earlier runs, `good` existed only in the dpdd view as the result of logical operations on several native-quantity flags).  We also have to exclude non-primary objects, as the dpdd view does.  The flag `detect_isprimary` is in the **position** table. The query for run 2.1i_dr1b_v1 can be written as shown in the next cell. Skip it if you're in a hurry; even with these techniques it still takes about 13 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'run21i_dr1b_v1'\n",
    "join_query = 'select count(position.object_id) from {schema}.position left join {schema}.dpdd_ref '\n",
    "join_query += 'on {schema}.position.object_id = {schema}.dpdd_ref.object_id'\n",
    "join_query = join_query.format(**locals())\n",
    "where = \" where detect_isprimary and good and (not deblend_skipped) and (ext_shapeHSM_HSmShapeRegauss_flag = 'f')\"\n",
    "join_query += where\n",
    "print(join_query)                # confirm the query looks reasonable\n",
    "with dbconn.cursor() as cursor:\n",
    "    %time cursor.execute(join_query)\n",
    "    for record in cursor:\n",
    "        print(record[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all such joins, **position** should be the first table in the join and left joins should be used, as in the example. This will in general result in better performance since it forces the join(s) to be done in the order specified. All tables are indexed on `object_id`. Only the **position** table has additional indexes (on `detect_isprimary` and on `coord`, a special column used in area searches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many math functions from the c library have been wrapped and incorporated in an extension module installed in the database. They have their normal c library names with the prefix c_. Functions with a floating point argument or return value usually have two versions, such as c_log (double precision natural logarithm) and c_logf (single precision). They can be incorporated in queries as in this example using the command-line interface program psql:\n",
    "```\n",
    "desc_dc2_drp=> select c_asin(1.0);\n",
    "\n",
    "     c_asin\n",
    "-----------------\n",
    " 1.5707963267949\n",
    "```\n",
    "There are also functions specially crafted for HSC or LSST catalogs with suggestive names like `patch_contains`, `tract_from_object_id` (used in q3 in the first notebook of this series), `sky_to_pixel`,..\n",
    "```\n",
    "desc_dc2_drp=> select count(*) from run12i.dpdd where tractsearch(objectId, 5063);\n",
    " count\n",
    "--------\n",
    " 233982\n",
    "(1 row)\n",
    "```\n",
    "### Restricting by tract or patch\n",
    "\n",
    "Let's try the last query from the previous section restriced to tract 3446 (picked at random), which has about 570,000 objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'run21i_dr1b_v1'\n",
    "join_query = 'select count(position.object_id) from {schema}.position left join {schema}.dpdd_ref '\n",
    "join_query += 'on {schema}.position.object_id = {schema}.dpdd_ref.object_id'\n",
    "join_query = join_query.format(**locals())\n",
    "where = \" where detect_isprimary and tractsearch(position.object_id, 3446) and  \"\n",
    "where += \"good and (not deblend_skipped) and (ext_shapeHSM_HSmShapeRegauss_flag = 'f')\"\n",
    "join_query += where\n",
    "print(join_query)                # confirm the query looks reasonable\n",
    "with dbconn.cursor() as cursor:\n",
    "    %time cursor.execute(join_query)\n",
    "    for record in cursor:\n",
    "        print(record[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tract 3446 has about 1/136 of the objects in the run21i_dr1b_v1 dataset. The elapsed time for the tract query (2 or 3 seconds) is much less than 1/136 of the time taken by the original query (780 seconds/ 136 = about 5.7 seconds). It pays to restrict queries to a tract when possible, even if it means issuing the same query for several tracts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Searches\n",
    "The  **dpdd** view has one extra column, `coord`, which is not formally a DPDD quantity. `coord` is an alternate way (other than `ra` and `dec`) to express location.  A `coord` value is a triple of doubles representing a position on a sphere in units of arcseconds. This column is indexed, which can make certain calculations faster. In particular, using the functions `conesearch` and `boxsearch` (which take a `coord` as input) rather than starting with `ra` and `dec` makes queries much faster.  There are also functions to translate between `coord` and `(ra, dec)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cone search\n",
    "Find all stars satisfying quality cuts within a fixed radius of a particular coordinate.  The function `coneSearch` returns true if `coord` is within the cone centered at (ra, dec) of the specified radius, measured in arcseconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'run21i_dr1b_v1'\n",
    "band = 'i'\n",
    "mag_col = 'mag_' + band\n",
    "min_SNR = 25   \n",
    "max_err = 1/min_SNR\n",
    "pop = 'Stars'\n",
    "ra = 54.5\n",
    "decl = -31.4\n",
    "radius = 240.0\n",
    "where = ' where (magerr_{band} < {max_err}) and clean and (extendedness < 1.0) and coneSearch(coord, {ra}, {decl}, {radius})'\n",
    "qcone = ('SELECT ra, dec, mag_{band} from {schema}.dpdd ' + where).format(**locals())\n",
    "print(qcone)\n",
    "with dbconn.cursor() as cursor:\n",
    "    %time cursor.execute(qcone)\n",
    "    records = cursor.fetchall()\n",
    "    nObj = len(records)\n",
    "    print('{} objects found '.format(nObj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmags = pd.DataFrame(records, columns=['ra', 'dec', mag_col])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('ra')\n",
    "plt.ylabel('dec')\n",
    "plt.suptitle(pop + ' Cone search', size='xx-large', y=0.92)\n",
    "p = plt.scatter(cmags['ra'], cmags['dec'], color='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how fast the query is.   Compare time, # objects found and scatter plot after increasing radius, e.g. by a factor of 10 to 2400.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box search\n",
    "Find all stars, subject to quality cuts, with the specified ra and dec bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra1 = 54.4\n",
    "ra2 = 54.8\n",
    "decl1 = -31.6\n",
    "decl2 = -31.3\n",
    "\n",
    "where = ' where (magerr_{band} < {max_err}) and clean and (extendedness < 1.0) and boxSearch(coord, {ra1}, {ra2},{decl1}, {decl2})'\n",
    "qbox = ('SELECT ra, dec, mag_{band} from {schema}.dpdd ' + where).format(**locals())\n",
    "print(qbox)\n",
    "with dbconn.cursor() as cursor:\n",
    "    %time cursor.execute(qbox)\n",
    "    records = cursor.fetchall()\n",
    "    nObj = len(records)\n",
    "    print('{} objects found '.format(nObj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmags = pd.DataFrame(records, columns=['ra', 'dec', mag_col])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('ra')\n",
    "plt.ylabel('dec')\n",
    "plt.suptitle(pop + ' Box search', size='xx-large', y=0.92)\n",
    "p = plt.scatter(bmags['ra'], bmags['dec'], color='y')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python",
   "language": "python",
   "name": "desc-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
