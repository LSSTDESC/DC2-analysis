{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f7f31a-fbcb-4d02-90c3-7314ec801aca",
   "metadata": {},
   "source": [
    "# Matching Halo Quantities from Skysim to CosmoDC2\n",
    "\n",
    "Here I show how to match the halos between cosmoDC2 and Skysim to correct for duplicate halos or to incoporate SOD quantities. I'm going to start with a caveat here: DC2 (the image simulation) exactly matches cosmoDC2, bugs and all. As the cosmoDC2 catalog doesn't incorporate halo shapes, the skysim shapes aren't carried through to DC2, and removing duplicates from cosmoDC2 is only recommended if you treat the corresponding DC2 area carefully. Proceed with caution!\n",
    "\n",
    "Owner: Patricia Larsen\n",
    "\n",
    "Last verified run: April 26, 2024 by @patricialarsen\n",
    "\n",
    "This notebook uses the Generic Catalog Reader (GCR, https://github.com/yymao/generic-catalog-reader) to access halo quantities. For more assistance with using GCR please see https://github.com/LSSTDESC/gcr-catalogs/blob/master/examples/GCRCatalogs%20Demo.ipynb\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "After working through and studying this Notebook you should be able to\n",
    "\n",
    "- Match cosmoDC2 halos to their skysim quantities\n",
    "- Get a list of duplicate halos from cosmoDC2\n",
    "\n",
    "\n",
    "Logistics: This notebook is intended to be run through the JupyterHub NERSC interface available here: https://jupyter.nersc.gov. To setup your NERSC environment, please follow the instructions available here: https://confluence.slac.stanford.edu/display/LSSTDESC/Using+Jupyter+at+NERSC\n",
    "\n",
    "**TLDR: you can skip most of the discussions, look at sections titled match skysim properties, or putting it all together for the codes needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7de9e674-f6df-4ac3-9f8e-edde9eeff2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import GCRCatalogs\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from GCR import GCRQuery\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09331a45-7eff-49d5-9d33-ad1b1d57666c",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "I'm going to load the data without much comment, for full details on these quantities and the read-in, please look at the extragalactic_halo_quantities tutorial. Notably though we're keeping the x,y,z, vx, vy, vz quantities, masses and simulation ID values \n",
    "to match (and test matching of) the catalogs, along with a couple of sod quantities. We are filtering by \"is_central\" to keep only the set of halo quantities and limiting ourselves to the small catalog and z<0.5 for this test. I've chosen not to filter on mass so that we can \n",
    "see the mass dependence of the duplicate issue and the SOD quantities. \n",
    "\n",
    "**Important note**\n",
    "Please make sure you're reading in the same sky area for each catalog. Use the _image or _small catalogs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6ae88157-ddf2-439a-986b-6240e4ed479f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc_cdc2 = GCRCatalogs.load_catalog('cosmoDC2_v1.1.4_small')\n",
    "gc_sky = GCRCatalogs.load_catalog('skysim5000_v1.2_small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3754230-8697-4845-a9cb-6dff3c56eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_data = gc_cdc2.get_quantities(['ra','dec', 'x', 'y', 'z', 'vx', 'vy', 'vz', 'redshift', 'halo_mass', 'halo_id','baseDC2/target_halo_mass','baseDC2/target_halo_id',\n",
    "                                      'baseDC2/target_halo_fof_halo_id', 'lightcone_replication', 'step'], \n",
    "                                 filters=['is_central','redshift < 0.5']) \n",
    "\n",
    "cluster_data_sky = gc_sky.get_quantities(['ra','dec', 'x', 'y', 'z',  'vx', 'vy', 'vz', 'redshift', 'halo_mass', 'halo_id','baseDC2/target_halo_mass','baseDC2/target_halo_id',\n",
    "                                       'baseDC2/target_halo_fof_halo_id', 'baseDC2/sod_halo_mass','baseDC2/sod_halo_cdelta',  'lightcone_replication', 'step'], \n",
    "                                 filters=['is_central','redshift < 0.5']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74814fa0-ac2f-4716-b0dc-cce7bdd30a14",
   "metadata": {},
   "source": [
    "### Match Skysim properties to CosmoDC2 halos \n",
    "\n",
    "The target_halo_fof_halo_id is the base ID from the simulation, so it can be used alongside the lightcone replication and the simulation step to match halo masses etc. You should be careful to match the replication and step and not just the tag. \n",
    "\n",
    "Let's assume a selection here of >5x10^14 Msun/h mass halos, just so we can look at the values more easily. First we must make a unique identifier for each halo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedcfe3-51f7-4cb1-8272-b555c617cf50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosmodc2_halos_tag = cluster_data['baseDC2/target_halo_fof_halo_id'][cluster_data['halo_mass']>5.e14]\n",
    "cosmodc2_halos_step = cluster_data['step'][cluster_data['halo_mass']>5.e14]\n",
    "cosmodc2_replication = cluster_data['lightcone_replication'][cluster_data['halo_mass']>5.e14]\n",
    "nhalos = len(cosmodc2_halos_tag)\n",
    "\n",
    "cosmodc2_unique_identifier = np.array([str(cosmodc2_halos_tag[i])+ str('_') + str(cosmodc2_halos_step[i])+ str('_') + str(cosmodc2_replication[i]) for i in range(nhalos)])\n",
    "\n",
    "skysim_halos_tag = cluster_data_sky['baseDC2/target_halo_fof_halo_id'][cluster_data_sky['halo_mass']>5.e14]\n",
    "skysim_halos_step = cluster_data_sky['step'][cluster_data_sky['halo_mass']>5.e14]\n",
    "skysim_replication = cluster_data_sky['lightcone_replication'][cluster_data_sky['halo_mass']>5.e14]\n",
    "nhalos = len(skysim_halos_tag)\n",
    "\n",
    "skysim_unique_identifier = np.array([str(skysim_halos_tag[i])+ str('_') + str(skysim_halos_step[i])+ str('_') + str(skysim_replication[i]) for i in range(nhalos)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f4b8b-9d3f-4646-8b0d-f8b67c33a056",
   "metadata": {},
   "source": [
    "Now *for the halos that are common across the simulations* (this is everything except the duplicate bug halos mentioned below, and for lower mass halos due to resolution issues or cuts), we can create a common ordering with numpy's intersect1d function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe3fb1-07e8-4e3f-a6e9-d123f010a36c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "identifiers, idx1, idx2 = np.intersect1d(cosmodc2_unique_identifier, skysim_unique_identifier,return_indices=True, assume_unique=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4487f-d194-43c0-a79d-16b5e41c6b00",
   "metadata": {},
   "source": [
    "We now confirm this is working as expected, and use it to plot the SOD/FOF mass relation for the (unduplicated) cosmoDC2 halos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236166f-3169-48a4-9167-3d05e14a7a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print((cosmodc2_unique_identifier[idx1]==skysim_unique_identifier[idx2]).all())\n",
    "print((cluster_data['halo_mass'][cluster_data['halo_mass']>5.e14][idx1] ==cluster_data_sky['halo_mass'][cluster_data_sky['halo_mass']>5.e14][idx2]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa98ee-24a1-4f54-874c-14866a2f493f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# h factor here is due to unit difference between native and non-native quantities. \n",
    "h = gc_sky.cosmology.H0.value/100.\n",
    "cosmodc2_halo_fof_masses = cluster_data['halo_mass'][cluster_data['halo_mass']>5.e14][idx1]\n",
    "cosmodc2_halo_sod_masses = cluster_data_sky['baseDC2/sod_halo_mass'][cluster_data_sky['halo_mass']>5.e14][idx2]/h\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(np.log10(cosmodc2_halo_fof_masses),np.log10(cosmodc2_halo_sod_masses))\n",
    "plt.plot(np.linspace(14.2,15.1,100),np.linspace(14.2,15.1,100),'k--')\n",
    "plt.xlabel(\"M_FOF (Msun/h)\")\n",
    "plt.ylabel(\"M_200c (Msun/h)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433ee8e-4c9d-48ea-826e-cfbc77ce5a63",
   "metadata": {},
   "source": [
    "### Find those duplicates!\n",
    "\n",
    "Let's start by showing how you can identify a rare issue that affected some cluster-mass halos in the cosmoDC2 catalog. \n",
    "\n",
    "First some background:\n",
    "\n",
    "In the merger-tree creation we often divide halos into fragments, to identify halos that may not fully merge in, but are temporarily associated with each other as they move past one another. Most high-mass halos have some number of fly-by events and so themselves are classified as fragment objects for the purposes of the merger tree. The most massive fragment of the total object will be given ownership of the whole FOF halo, and the position as it crosses the lightcone will be determined by that fragment's location. \n",
    "\n",
    "Now, in creating the lightcone we run on a large number of ranks, and sometimes fragments of the same object will live on different ranks. In rare cases you might have two equal-mass fragments in which case the decision of the most-massive fragment is arbitrary, and so when deciding if the halo should exist on this rank or its neighbor, the different ranks didn't always make the same decision and both ranks took ownership, creating a duplicate halo.   \n",
    "\n",
    "One way to look at the uniqueness of halos is to look at the fof halo IDs from the simulation. However you have to be a little careful with lightcone halos, as the fof halo IDs are only unique for a given redshift step and box replication. But if we look at the masses and positions and they are both equal this duplication has occured. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97630ac-245f-48c2-812c-395bba036e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val, counts = np.unique(cluster_data['halo_mass'][cluster_data['halo_mass']>5.e14],return_counts=True)\n",
    "print(val[counts==2][0])\n",
    "print(\" \")\n",
    "print(\"Properties of potential duplicate halo\")\n",
    "print(\"--------------------------------------\")\n",
    "mask_duplicate = cluster_data['halo_mass']==val[counts==2][0]\n",
    "for key in ['x','y','z']:\n",
    "    print(key,cluster_data[key][mask_duplicate])\n",
    "for key in ['vx','vy','vz']:\n",
    "    print(key,cluster_data[key][mask_duplicate])\n",
    "print(\" \")\n",
    "print(\"Masses of potential duplicate halo\")\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "for key in ['halo_mass','baseDC2/target_halo_mass']:\n",
    "    print(key,cluster_data[key][mask_duplicate])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52601a5-fd13-411a-bff5-a7fcac3cce2f",
   "metadata": {},
   "source": [
    "These are around 100kpc apart, consistent with fragments of the same halo. The velocities differ a little as they're different fragments, and the masses are exactly the same (note unit differences between the two measurements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd1158c-9110-4062-a37d-67b18df1b2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val, counts = np.unique(cluster_data['halo_mass'][cluster_data['halo_mass']>5.e14],return_counts=True)\n",
    "print(val[counts==2][0])\n",
    "print(\" \")\n",
    "print(\"ID values of potential duplicate halo\")\n",
    "print(\"--------------------------------------\")\n",
    "mask_duplicate = cluster_data['halo_mass']==val[counts==2][0]\n",
    "for key in ['halo_id','baseDC2/target_halo_id','baseDC2/target_halo_fof_halo_id']:\n",
    "    print(key,cluster_data[key][mask_duplicate])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1064c59-e2e8-4c42-85d5-c723d027e7ce",
   "metadata": {},
   "source": [
    "Note that these have unique ID values (and that the target_halo_id and fof_tag are somewhat different). This means that the IDs can't be found using cosmoDC2 data alone, except by mass/position matches. However we can use skysim data to identify them.. \n",
    "\n",
    "\n",
    "Let's look at the same properties for the Skysim catalog, when we look for the same mass we only get one object, with the same properties as one of the cosmoDC2 objects. This is the bug-corrected catalog where this duplicate doesn't exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe96c5-5295-4ce2-85c9-25ee025687a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\" \")\n",
    "print(\"Properties of potential duplicate halo (skysim)\")\n",
    "print(\"--------------------------------------\")\n",
    "mask_duplicate = cluster_data_sky['halo_mass']==val[counts==2][0]\n",
    "for key in ['x','y','z']:\n",
    "    print(key,cluster_data_sky[key][mask_duplicate])\n",
    "for key in ['vx','vy','vz']:\n",
    "    print(key,cluster_data_sky[key][mask_duplicate])\n",
    "print(\" \")\n",
    "print(\"Masses of potential duplicate halo\")\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "for key in ['halo_mass','baseDC2/target_halo_mass']:\n",
    "    print(key,cluster_data_sky[key][mask_duplicate])\n",
    "print(\" \")\n",
    "print(\"ID values of potential duplicate halo\")\n",
    "print(\"--------------------------------------\")\n",
    "\n",
    "mask_duplicate = cluster_data_sky['halo_mass']==val[counts==2][0]\n",
    "for key in ['halo_id','baseDC2/target_halo_id','baseDC2/target_halo_fof_halo_id']:\n",
    "    print(key,cluster_data_sky[key][mask_duplicate])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba395a-2e3b-4246-a5f0-b987c5268dff",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "So the idea is that we can use the skysim catalog as a check to see if the halo should exist or not and get a list of \"fake\" halos. We have to be careful of three things that can affect our results though:\n",
    "\n",
    "- The fof halo tag of a \"bad\" object may exist in a different lightcone replication or redshift of the skysim catalog\n",
    "- Mass cuts, or other imposed cuts on galaxy luminosity can prevent a \"good\" halo tag from existing in skysim\n",
    "\n",
    "Keeping those in mind, let's add the step and replication to the inputs (with the benefit of hindsight they're already in memory) and try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27304df3-f97d-4832-838a-b25a5dd2aa41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(' ')\n",
    "for item in gc_cdc2.list_all_native_quantities():\n",
    "    if 'rep' in item:\n",
    "        print(item)\n",
    "print(' ')\n",
    "for item in gc_cdc2.list_all_native_quantities():\n",
    "    if 'step' in item:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdd323-90ed-40d8-bc4b-5c21c7dc67bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_ids = np.array([])\n",
    "bad_reps = np.array([])\n",
    "bad_steps =  np.array([])\n",
    "\n",
    "replication_list = np.unique(cluster_data['lightcone_replication'])\n",
    "step_list = np.unique(cluster_data['step'])\n",
    "\n",
    "for rep in replication_list:\n",
    "    for step in step_list:\n",
    "        mask_sample = (cluster_data['lightcone_replication']==rep)&(cluster_data['step']==step)\n",
    "        mask_sample_sky = (cluster_data_sky['lightcone_replication']==rep)&(cluster_data_sky['step']==step)\n",
    "        fof_tags = cluster_data['baseDC2/target_halo_fof_halo_id'][mask_sample]\n",
    "        fof_tags_sky = cluster_data_sky['baseDC2/target_halo_fof_halo_id'][mask_sample_sky]\n",
    "        \n",
    "        mask_bad = np.logical_not(np.isin(fof_tags,fof_tags_sky))\n",
    "        bad_ids = np.concatenate((fof_tags[mask_bad],bad_ids))\n",
    "        bad_reps = np.concatenate((np.ones(np.sum(mask_bad),dtype='int')*rep,bad_reps))\n",
    "        bad_steps = np.concatenate((np.ones(np.sum(mask_bad),dtype='int')*step,bad_steps))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac0a7c-f92b-44f6-b8bc-686a06415ce7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### But wait there's more!\n",
    "\n",
    "The bad ids here are actually a combination of duplicates and IDs that aren't found in skysim due to cuts, so let's look at them more closely. First let's actually check from cosmoDC2 if there is another object at the same step and replication with equal mass, if so we put it in the list...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d1c8c-caf3-4f29-94d5-cba16a9beb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of candidate duplicates: \", len(bad_ids))\n",
    "duplicates=0\n",
    "\n",
    "duplicate_masses = []\n",
    "duplicate_halo_id = []\n",
    "\n",
    "bad_ids_true=[]\n",
    "bad_reps_true=[]\n",
    "bad_steps_true=[]\n",
    "dists=[]\n",
    "\n",
    "\n",
    "# I choose to keep a pretty wide range of possible matching distances for this, however note that very low mass halos may be physically close \n",
    "# and have equal masses from discretization\n",
    "\n",
    "for obj in range(len(bad_ids)):\n",
    "    #find the halo\n",
    "    idx = np.where((cluster_data['baseDC2/target_halo_fof_halo_id']==bad_ids[obj])&(cluster_data['lightcone_replication']==bad_reps[obj])&(cluster_data['step']==bad_steps[obj]))[0][0]\n",
    "    # get a comparison sample of objects with the same replication number and step\n",
    "    mask_comparison = (cluster_data['lightcone_replication']==bad_reps[obj])&(cluster_data['step']==bad_steps[obj])\n",
    "    # calculate distance of halo sample to halo \n",
    "    dist_halos = np.sqrt((cluster_data['x'][mask_comparison] - cluster_data['x'][idx])**2 + (cluster_data['y'][mask_comparison] - cluster_data['y'][idx])**2 + (cluster_data['z'][mask_comparison] - cluster_data['z'][idx])**2)\n",
    "    # calculate whether mass and distance are both close (exact mass and dist<200kpc)\n",
    "    mask_dup = (cluster_data['halo_mass'][mask_comparison]==cluster_data['halo_mass'][idx])&(dist_halos<0.2)\n",
    "    if np.sum(mask_dup)>1:\n",
    "        # we should have one duplicate exactly (matching to itself), so duplicate means >1\n",
    "        bad_ids_true.append(bad_ids[obj])\n",
    "        bad_reps_true.append(bad_reps[obj])\n",
    "        bad_steps_true.append(bad_steps[obj])\n",
    "        duplicate_masses.append(cluster_data['halo_mass'][idx])\n",
    "        duplicate_halo_id.append(cluster_data['halo_id'][idx])\n",
    "        dists.append(dist_halos[mask_dup][dist_halos[mask_dup]>0])\n",
    "        duplicates+=1\n",
    "        \n",
    "print(\"Number of final duplicates: \", duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c36df-7214-4cdd-97b5-2f6e66f91962",
   "metadata": {},
   "source": [
    "We see here that low mass halos have a larger separation, especially as the size of a low-mass halo should be pretty low, so these are likely not true duplicates.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231aa11-c468-4eef-82f3-fc4669db3fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.log10(duplicate_masses), np.array(dists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c440f-d45e-41dc-9ea6-e3c642e5240e",
   "metadata": {},
   "source": [
    "## Final method:\n",
    "\n",
    "The best way to check if this is a true duplicate is to confirm that skysim finds objects corresponding to these duplicate candidates - it shouldn't have the exact same object (as this is the definition of duplicate we've enforced), so if there is an object of the same halo mass which is physically close, it must have been a duplicate.\n",
    "\n",
    "We find that the 5 high mass objects here are true duplicates (i.e. have a corresponding pair in skysim but have a missing ID for being a duplicate), and the lower mass objects are simply neighbouring low-mass halos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60beb0-d523-4bd5-9780-09881281382a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of candidate duplicates: \", len(bad_ids))\n",
    "duplicates=0\n",
    "\n",
    "duplicate_masses = []\n",
    "duplicate_halo_id = []\n",
    "bad_ids_true=[]\n",
    "bad_reps_true=[]\n",
    "bad_steps_true=[]\n",
    "dists=[]\n",
    "\n",
    "for obj in range(len(bad_ids)):\n",
    "    #find the halo\n",
    "    idx = np.where((cluster_data['baseDC2/target_halo_fof_halo_id']==bad_ids[obj])&(cluster_data['lightcone_replication']==bad_reps[obj])&(cluster_data['step']==bad_steps[obj]))[0][0]\n",
    "    # get a comparison sample of objects with the same replication number and step\n",
    "    mask_comparison = (cluster_data_sky['lightcone_replication']==bad_reps[obj])&(cluster_data_sky['step']==bad_steps[obj])\n",
    "    # calculate distance of halo sample to halo \n",
    "    dist_halos = np.sqrt((cluster_data_sky['x'][mask_comparison] - cluster_data['x'][idx])**2 + (cluster_data_sky['y'][mask_comparison] - cluster_data['y'][idx])**2 + (cluster_data_sky['z'][mask_comparison] - cluster_data['z'][idx])**2)\n",
    "    # calculate whether mass and distance are both close (exact mass and dist<200kpc)\n",
    "    mask_dup = (cluster_data_sky['halo_mass'][mask_comparison]==cluster_data['halo_mass'][idx])&(dist_halos<0.2)\n",
    "    #print(obj, np.sum(cluster_data_sky['halo_mass'][mask_comparison]==cluster_data['halo_mass'][idx]))\n",
    "    if np.sum(mask_dup)>0:\n",
    "        # we should have one duplicate exactly (matching to itself), so duplicate means >1\n",
    "        bad_ids_true.append(bad_ids[obj])\n",
    "        bad_reps_true.append(bad_reps[obj])\n",
    "        bad_steps_true.append(bad_steps[obj])\n",
    "        duplicate_masses.append(cluster_data['halo_mass'][idx])\n",
    "        duplicate_halo_id.append(cluster_data['halo_id'][idx])\n",
    "        dists.append(dist_halos[mask_dup][dist_halos[mask_dup]>0])\n",
    "        duplicates+=1\n",
    "\n",
    "\n",
    "        \n",
    "print(\"Number of duplicates found in skysim: \", duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1261d45-9687-488b-9de5-23f3ec1cb1aa",
   "metadata": {},
   "source": [
    "## Final Steps\n",
    "\n",
    "Now we have our final list of duplicates, let's take a look at them. Firstly these are all high-mass halos, very much in the cluster regime. This makes sense as the halo will need to cross rank boundaries for the duplicate bug to occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342385f-0afd-428c-8a0f-2b68ab3d0991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(np.log10(cluster_data_sky['halo_mass']), bins = np.linspace(10,15.5,100))\n",
    "plt.hist(np.log10(duplicate_masses), bins = np.linspace(10,15.5,100))\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "print(\"Percentage of > 1.e14 Msun/h mass halos affected = \", len(duplicate_masses)/np.sum(cluster_data_sky['halo_mass']>1.e14)*100., \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8a7df-dcdb-4ede-84aa-13ce63b6a7aa",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's put all the important code to output the halo lists together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309ee5d-10a0-4220-b3ad-d92044f42f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc_cdc2 = GCRCatalogs.load_catalog('cosmoDC2_v1.1.4_small')\n",
    "gc_sky = GCRCatalogs.load_catalog('skysim5000_v1.2_small')\n",
    "\n",
    "cluster_data = gc_cdc2.get_quantities(['ra','dec', 'x', 'y', 'z', 'vx', 'vy', 'vz', 'redshift', 'halo_mass', 'halo_id','baseDC2/target_halo_mass','baseDC2/target_halo_id',\n",
    "                                      'baseDC2/target_halo_fof_halo_id', 'lightcone_replication', 'step'], \n",
    "                                 filters=['is_central','redshift < 0.5']) \n",
    "\n",
    "cluster_data_sky = gc_sky.get_quantities(['ra','dec', 'x', 'y', 'z',  'vx', 'vy', 'vz', 'redshift', 'halo_mass', 'halo_id','baseDC2/target_halo_mass','baseDC2/target_halo_id',\n",
    "                                       'baseDC2/target_halo_fof_halo_id', 'baseDC2/sod_halo_mass','baseDC2/sod_halo_cdelta',  'lightcone_replication', 'step'], \n",
    "                                 filters=['is_central','redshift < 0.5']) \n",
    "\n",
    "bad_ids = np.array([])\n",
    "bad_reps = np.array([])\n",
    "bad_steps =  np.array([])\n",
    "\n",
    "replication_list = np.unique(cluster_data['lightcone_replication'])\n",
    "step_list = np.unique(cluster_data['step'])\n",
    "\n",
    "for rep in replication_list:\n",
    "    for step in step_list:\n",
    "        mask_sample = (cluster_data['lightcone_replication']==rep)&(cluster_data['step']==step)\n",
    "        mask_sample_sky = (cluster_data_sky['lightcone_replication']==rep)&(cluster_data_sky['step']==step)\n",
    "        fof_tags = cluster_data['baseDC2/target_halo_fof_halo_id'][mask_sample]\n",
    "        fof_tags_sky = cluster_data_sky['baseDC2/target_halo_fof_halo_id'][mask_sample_sky]\n",
    "        \n",
    "        mask_bad = np.logical_not(np.isin(fof_tags,fof_tags_sky))\n",
    "        bad_ids = np.concatenate((fof_tags[mask_bad],bad_ids))\n",
    "        bad_reps = np.concatenate((np.ones(np.sum(mask_bad),dtype='int')*rep,bad_reps))\n",
    "        bad_steps = np.concatenate((np.ones(np.sum(mask_bad),dtype='int')*step,bad_steps))\n",
    "        \n",
    "        \n",
    "print(\"Number of candidate duplicates: \", len(bad_ids))\n",
    "duplicates=0\n",
    "\n",
    "duplicate_masses = []\n",
    "duplicate_halo_id = []\n",
    "bad_ids_true=[]\n",
    "bad_reps_true=[]\n",
    "bad_steps_true=[]\n",
    "dists=[]\n",
    "\n",
    "for obj in range(len(bad_ids)):\n",
    "    #find the halo\n",
    "    idx = np.where((cluster_data['baseDC2/target_halo_fof_halo_id']==bad_ids[obj])&(cluster_data['lightcone_replication']==bad_reps[obj])&(cluster_data['step']==bad_steps[obj]))[0][0]\n",
    "    # get a comparison sample of objects with the same replication number and step\n",
    "    mask_comparison = (cluster_data_sky['lightcone_replication']==bad_reps[obj])&(cluster_data_sky['step']==bad_steps[obj])\n",
    "    # calculate distance of halo sample to halo \n",
    "    dist_halos = np.sqrt((cluster_data_sky['x'][mask_comparison] - cluster_data['x'][idx])**2 + (cluster_data_sky['y'][mask_comparison] - cluster_data['y'][idx])**2 + (cluster_data_sky['z'][mask_comparison] - cluster_data['z'][idx])**2)\n",
    "    # calculate whether mass and distance are both close (exact mass and dist<200kpc)\n",
    "    mask_dup = (cluster_data_sky['halo_mass'][mask_comparison]==cluster_data['halo_mass'][idx])&(dist_halos<0.2)\n",
    "    #print(obj, np.sum(cluster_data_sky['halo_mass'][mask_comparison]==cluster_data['halo_mass'][idx]))\n",
    "    if np.sum(mask_dup)>0:\n",
    "        # we should have one duplicate exactly (matching to itself), so duplicate means >1\n",
    "        bad_ids_true.append(bad_ids[obj])\n",
    "        bad_reps_true.append(bad_reps[obj])\n",
    "        bad_steps_true.append(bad_steps[obj])\n",
    "        duplicate_masses.append(cluster_data['halo_mass'][idx])\n",
    "        duplicate_halo_id.append(cluster_data['halo_id'][idx])\n",
    "        dists.append(dist_halos[mask_dup][dist_halos[mask_dup]>0])\n",
    "        duplicates+=1\n",
    "\n",
    "\n",
    "        \n",
    "print(\"Number of duplicates found in skysim: \", duplicates)\n",
    "print(\"Halo ids of duplicate halos: \", duplicate_halo_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef0483-884d-4600-a8bc-e55dd717159d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157218c2-6b09-4f3f-b3bb-166895e1292b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python",
   "language": "python",
   "name": "desc-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
