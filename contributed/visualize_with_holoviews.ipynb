{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2.2i Object Tables with Dask+Holoviews\n",
    "<br>Owner: **Michael Wood-Vasey** ([@wmwv](https://github.com/LSSTDESC/DC2-analysis/issues/new?body=@wmwv))\n",
    "<br>Last Verified to Run: **2020-07-09**\n",
    "    \n",
    "* Demonstrate the use of the DPDD-style Object table in Parquet format. \n",
    "* Uses Dask+Holoviews+Datashader to demonstrate visualizing the entire dataset.\n",
    "\n",
    "Learning Objectives:\n",
    "After completing and studying this Notebook, a person should be able to\n",
    "1. Load a Parquet file with Dask.\n",
    "    - Understand that specifying the columns to select increases performance.\n",
    "2. Make a plot using Holoviews\n",
    "3. Use Datashader to interactively rasterize a large dataset for display.\n",
    "4. Launch a set of Dask workers on a SLURM qos=interactive job.\n",
    "\n",
    "Logistics:\n",
    "\n",
    "1. These tests were conducted on NERSC through the https://jupyter.nersc.gov interface.\n",
    "\n",
    "2. Requires:\n",
    "```\n",
    "dask\n",
    "dask.distributed\n",
    "holoviews\n",
    "datashader\n",
    "bokeh\n",
    "pyarrow >= 0.13.1\n",
    "```\n",
    "\n",
    "These were used with `desc-python-bleed` kernel\n",
    "\n",
    "3. This was run using the `desc-python-bleed` kernel\n",
    "\n",
    "Current Status:\n",
    "* Quick demonstration for people who want to check out the Parquet file and learn.\n",
    "* Future work planned to demonstrate selection, brushing, and simple outlier identification.\n",
    "\n",
    "References:  \n",
    "    https://dask.org  \n",
    "    https://datashader.org  \n",
    "    https://parquet.apache.org  \n",
    "    https://holoviews.org  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "Parquet is a column-based storage format that's part of a wider Arrow project to provide standardized, high-performance data representations in memory and on disk.  It's commonly used in current data science and large data volume processing, and is the current selected standard for Rubin Observatory LSST Data Management on-disk representations of output data catalogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask\n",
    "\n",
    "Dask allows us to do processing by dividing tasks into individual workers.  These workers allow us to take fuller use of available memory and processors, including those on other machines.\n",
    "\n",
    "Dask is solving the needs to:\n",
    "\n",
    "1. Load more data than fit into memory. You can delay this in either time or space.\n",
    "   * Delaying in time would be if you running on a memory-limited machine, then Dask will be able to chunk through the work units without simultaneously needing the full amount of memory to hold all of the data at once.\n",
    "   * Delaying in space means spinning up additional machines.  This is often particularly powerful when connecting your front-end machine (e.g., a NERSC JupyterHub job is limited to 42 GB memory), to several full cluster compute nodes (e.g., a NERSC Haswell node is 32 real cores, 128 GB).  \n",
    "\n",
    "2. Distributing work across multiple processors. Python and numpy/scipy are not naturally parallel or easily parallelizable. One of the common things we will do with large datasets is aggregate for both analysis and visualization. Being able to do this aggregation in parallel is a significant gain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoloViews\n",
    "\n",
    "HoloViews is Dask aware and can provide Dask the correct information to build a Task Graph that effectively parallelizes the requisite data loading and computation.  HoloViews can use either bokeh or matplotlib backends.  If you directly use the matplotlib backend with a Dask DataFrame it will not appropriately parallelize across the workers and instead do lots of stuff in serial.  Bokeh also gives some nice interactive capabilities and HoloViews knows how to appropriate set up the linking and call backs to enable coordinated zooming and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here specify that we want to use HoloViews with the Bokeh backend.  You could also use a matplotlib backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start our Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple testing and illustration of how to use dask, holoview, and datashader here you can run locally on just one tract.\n",
    "To run on the full set of DR6, you'll need to set up a node to support Dask distributed.\n",
    "\n",
    "Dask allows us to shift processing in either time or space (other machines).  Basically you just need a machine that can hold the data in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Dask Cluster on an Interactive Node\n",
    "\n",
    "Move to your SCRATCH directory to simplify file locking.  (On NERSC, the `SCRATCH` environment variable points to an individual SCRATCH area for each user.)\n",
    "```\n",
    "cd $SCRATCH/\n",
    "```\n",
    "\n",
    "In a separate Terminal on Cori (a JupyterHub Terminal works fine), ask for a Node from the `interactive` queue.  This generally completes in seconds.\n",
    "```\n",
    "salloc -N 1 -C haswell --qos=interactive -t 04:00:00\n",
    "```\n",
    "\n",
    "And then once on that Node, load the right Python environment:\n",
    "```\n",
    "python /global/common/software/lsst/common/miniconda/start-kernel-cli.py desc-python-bleed\n",
    "```\n",
    "\n",
    "And then start up the Dask Cluster\n",
    "```\n",
    "NUM_WORKERS=16\n",
    "SCHEDULER_FILE=${SCRATCH}/scheduler.json\n",
    "rm -rf ${SCHEDULER_FILE}\n",
    "dask-scheduler --scheduler-file ${SCHEDULER_FILE} &\n",
    "dask-worker --nprocs ${NUM_WORKERS} --scheduler-file ${SCHEDULER_FILE} &\n",
    "```\n",
    "\n",
    "(We explicitly `rm -rf ${SCHEDULER_FILE}` above in case it's still around from a previous invocation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You connect to this Dask cluster by passing in the location of the `SCHEDULER_FILE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_file = os.path.join(os.environ[\"SCRATCH\"], \"scheduler.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure the dashboard URL to use the JupyterHub proxy service.\n",
    "We here set the formatting string template to the correct value.\n",
    "Once we actually connect the client, the client can then tell us the full link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.config[\"distributed\"][\"dashboard\"][\"link\"] = \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{host}:{port}/status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(scheduler_file=scheduler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link after `Dashboard:` above to get a visualization of the Dask Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and (lazy-)load our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a directory path that contains all of the individual Parquet files.  If we want to read just one tract, we could pass the full path name of just the tract.  If we wanted to specify a set of tracts to run, we could pass in a regex that matches those tracts.  Or we could pass in a list that contained the file paths or directory paths to read from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_release = \"dr6a\"\n",
    "data_path = f\"/global/cfs/cdirs/lsst/shared/DC2-prod/Run2.2i/dpdd/Run2.2i-{data_release}/dc2_object_run2.2i_{data_release}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a column-based storage format.  If we specify the columns we want to load, then we avoid the memory overhead of having to store the other columns.\n",
    "\n",
    "Note: In principle Dask should be able to figure out what columns we actually used and only used those.\n",
    "    1. This just doesn't actually work using the `pyarrow` engine.  It loads the entire set of columns.\n",
    "    2. If we have enough memory to do so, we want to persist the subset of columns that we do use in memory.  Dask is going to compute a new task graph for each plot and if we don't help it out and tell it that we want to keep those columns around, it would otherwise completely re-read all of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns we need.  This allows for significant performance advantages when reading a column-based storage format such as Parquet.\n",
    "columns_to_read = ['ra', 'dec', 'mag_g', 'mag_r', 'mag_i',\n",
    "                   'magerr_g', 'magerr_r', 'magerr_i', 'extendedness']\n",
    "df = dd.read_parquet(data_path, columns=columns_to_read, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the data in Dask Cluster Worker memory\n",
    "\n",
    "Dask actively purges data from memory when its no longer needed by the Dask Task Graph currently doing the computation.\n",
    "\n",
    "That's not what we want here where we want to plot several quantities repeatedly.  So we explicitly tell Dask to persist this data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Create Color Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "snr_magerr_threshold = 0.3  # mag\n",
    "df = df[(df.magerr_g < snr_magerr_threshold) & (df.magerr_r < snr_magerr_threshold) & (df.magerr_i < snr_magerr_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['g-r'] = df['mag_g'] - df['mag_r']\n",
    "df['r-i'] = df['mag_r'] - df['mag_i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal = df[(df.extendedness > 0.95)]\n",
    "star = df[(df.extendedness < 0.95)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HoloViews `Points` objects and Wrap with Datashader\n",
    "\n",
    "Create for position, magnitude and color.  Use datashader to provide rasterized images that display in finite time but are still zoomable.\n",
    "\n",
    "We will first define several different HoloViews objects\n",
    "Then we will use the `+` overloading to display the visualizations.\n",
    "\n",
    "For some more examples and information, see\n",
    "https://holoviews.org/user_guide/Large_Data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_ra_dec = hv.Points(df, kdims=['ra', 'dec'])\n",
    "ra_dec = datashade(points_ra_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_mag_magerr = hv.Points(df, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                         hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "mag_magerr = datashade(points_mag_magerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_color_mag = hv.Points(df, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "color_mag = datashade(points_color_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_color_color = hv.Points(df, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                          hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "color_color = datashade(points_color_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Holoviews one uses the `+` operator to put these three different visualizations next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_magerr + color_mag + color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color-magnitude and color-color plots will zoom together.  The synchronized range zooming is just based on matching the range of g-r. it's not subsetting the points in the view.  Thus there is no relationship between the RA, Dec plot and the other plots.\n",
    "\n",
    "Related to this, I don't know how to invert the mag_g axis in the middle plot without also inverting the mag_g axis in the left plot and without breaking the shared axes selections.  One can `color_mag.opts(invert_yaxis=True)`, but that would not just invert the `mag_magerr` plot axis, which would be annoying, but even worse it would invert the linked selection to be 28 < mag_g < 14, which is the empty set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashade Multiple Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how to separate by `extendedness` parameter.  Note that `extendedness` is pretty conservative with a high false negative at faint magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to force the range for magerr with 'range' instead of 'soft_range'.  I'm not sure why.\n",
    "gal_mag_magerr = hv.Points(gal, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                       hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "gal_color_mag = hv.Points(gal, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                      hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "gal_color_color = hv.Points(gal, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "\n",
    "star_mag_magerr = hv.Points(star, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                       hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "star_color_mag = hv.Points(star, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "star_color_color = hv.Points(star, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                          hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "\n",
    "typed_mag_magerr = {'gal': gal_mag_magerr, 'star': star_mag_magerr}\n",
    "typed_color_mag = {'gal': gal_color_mag, 'star': star_color_mag}\n",
    "typed_color_color = {'gal': gal_color_color, 'star': star_color_color}\n",
    "\n",
    "shaded_mag_magerr = datashade(hv.NdOverlay(typed_mag_magerr, kdims='type'),\n",
    "                              aggregator=ds.count_cat('type'))\n",
    "shaded_color_mag = datashade(hv.NdOverlay(typed_color_mag, kdims='type'),\n",
    "                             aggregator=ds.count_cat('type'))\n",
    "shaded_color_color = datashade(hv.NdOverlay(typed_color_color, kdims='type'),\n",
    "                               aggregator=ds.count_cat('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaded_mag_magerr + shaded_color_mag + shaded_color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The galaxies are red, while the \"stars\" (== not obviously extended) are blue.\n",
    "\n",
    "Zoom in to mag - magerr plot to see that the outlying cluster of higher uncertainties as a function of magnitude are galaxies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "1. Constructing a legend for the above is unfortunately a little unobvious and awkward.  We lost the information when we datashaded the NdOverlay.  You could do it by creating a new set of empty NdOverlay object to get the colors. \n",
    "\n",
    "This is relying on the fact that the above commands used the default color map.  \n",
    "Explicitly specifying the color above would have been better.\n",
    "```\n",
    "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
    "color_key = {k: Sets1to3[i] for i, k in enumerate(typed_color_mag)}\n",
    "color_points = hv.NdOverlay({k: hv.Points([gal['g-r'][0], gal['mag_g'][0]],\n",
    "                                          label=str(k)).options(color=v) for k, v in color_key.items()})\n",
    "                                          \n",
    "shaded_color_mag * color_points + shaded_color_color * color_points\n",
    "```\n",
    "\n",
    "Above code adapted from http://holoviews.org/user_guide/Large_Data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use hover-over aggregation\n",
    "\n",
    "We can set up a dynamic hover-over that gives information about the local area.  In this case we're just doing a count of the number of points in a given rectangular region.\n",
    "\n",
    "Note the use of the `*` to compose the results of `datashade` and `hv.util.Dynamic`.  This is the idiom in Holoviews to combine several different visualizations/tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.streams import RangeXY\n",
    "\n",
    "# A funnily-named wrapper function to generate hover-overs by count.\n",
    "# nx, ny are fixed by the original range.  We don't get finer resolution as we zoom in.\n",
    "def dynamate(points, width=400, height=400, nx=50, ny=50):\n",
    "    \"\"\"Datashades points at width, height.  Hover-over in dynamic boxes of nx x ny at given display size\"\"\"\n",
    "    datashaded_points = datashade(points, width=width, height=height)\n",
    "    hover_over_count = \\\n",
    "        hv.util.Dynamic(rasterize(points, width=nx, height=ny, streams=[RangeXY]),\n",
    "                        operation=hv.QuadMesh)\n",
    "    return datashaded_points * hover_over_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_mag_magerr = hv.Points(df, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                         hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "mag_magerr = datashade(points_mag_magerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
    "dynamic_mag_magerr = dynamate(points_mag_magerr)\n",
    "dynamic_color_mag = dynamate(points_color_mag)\n",
    "dynamic_color_color = dynamate(points_color_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_mag_magerr + dynamic_color_mag + dynamic_color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut down Dask Cluster\n",
    "\n",
    "When you're done, go back to your Terminal window and log out of the interactive node.  This will both shut down the Dask Cluster and log out of your interactive node job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python-bleed",
   "language": "python",
   "name": "desc-python-bleed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
