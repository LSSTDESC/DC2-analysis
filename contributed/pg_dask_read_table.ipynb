{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2.2i Object Tables via PostgreSQL with Dask read_sql_table +Holoviews\n",
    "<br>Owner: **Joanne Bogart** ([@jrbogart](https://github.com/LSSTDESC/DC2-analysis/issues/new?body=@wmwv))\n",
    "<br>Last Verified to Run: 2020-08-12\n",
    "    \n",
    "* Demonstrate the use of the DPDD-style Object table stored in PostgreSQL db. \n",
    "* Uses Dask+Holoviews+Datashader to demonstrate visualizing the entire dataset.\n",
    "* Largely stolen from @wmwv \"Run 2.2i Object Tables with Dask+Holoviews\"\n",
    "\n",
    "Learning Objectives:\n",
    "After completing and studying this Notebook, a person should be able to\n",
    "1. Access database tables with Dask.\n",
    "    - Understand that specifying the columns to select increases performance.\n",
    "2. Make a plot using Holoviews\n",
    "3. Use Datashader to interactively rasterize a large dataset for display.\n",
    "4. Launch a set of Dask workers on a SLURM qos=interactive job.\n",
    "\n",
    "Logistics:\n",
    "\n",
    "1. These tests were conducted on NERSC through the https://jupyter.nersc.gov interface.\n",
    "\n",
    "2. Requires:\n",
    "```\n",
    "dask\n",
    "dask.distributed\n",
    "holoviews\n",
    "datashader\n",
    "bokeh\n",
    "```\n",
    "\n",
    "These were used with `desc-python-bleed` kernel\n",
    "\n",
    "3. This was run using the `desc-python-bleed` kernel\n",
    "\n",
    "Current Status:\n",
    "* Quick demonstration for people who want to check out use of PostgreSQL with DASK.\n",
    "* Future work planned: try matching Run2.2i dr6c with truth\n",
    "\n",
    "References:  \n",
    "    https://dask.org  \n",
    "    https://datashader.org   \n",
    "    https://holoviews.org  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start our Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple testing and illustration of how to use dask, holoview, and datashader here you can run locally on just one tract.\n",
    "To run on the full set of DR6, you'll need to set up a node to support Dask distributed.  Basically you just need a machine that can hold the data in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Dask Cluster on an Interactive Node\n",
    "\n",
    "Move to your SCRATCH directory to simplify file locking.  (On NERSC, the `SCRATCH` environment variable points to an individual SCRATCH area for each user.)\n",
    "```\n",
    "cd $SCRATCH/\n",
    "```\n",
    "\n",
    "In a separate Terminal on Cori (a JupyterHub Terminal works fine), ask for a Node from the `interactive` queue.  This generally completes in seconds.\n",
    "```\n",
    "salloc -N 1 -C haswell --qos=interactive -t 04:00:00\n",
    "```\n",
    "\n",
    "And then once on that Node, load the right Python environment:\n",
    "```\n",
    "python /global/common/software/lsst/common/miniconda/start-kernel-cli.py desc-python-bleed\n",
    "```\n",
    "\n",
    "And then start up the Dask Cluster\n",
    "```\n",
    "NUM_WORKERS=16\n",
    "SCHEDULER_FILE=${SCRATCH}/scheduler.json\n",
    "rm -rf ${SCHEDULER_FILE}\n",
    "dask-scheduler --scheduler-file ${SCHEDULER_FILE} &\n",
    "dask-worker --nprocs ${NUM_WORKERS} --scheduler-file ${SCHEDULER_FILE} &\n",
    "```\n",
    "\n",
    "(We explicitly `rm -rf ${SCHEDULER_FILE}` above in case it's still around from a previous invocation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You connect to this Dask cluster by passing in the location of the `SCHEDULER_FILE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_file = os.path.join(os.environ[\"SCRATCH\"], \"dask/scheduler.json\")  # modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure the dashboard URL to use the JupyterHub proxy service.\n",
    "We here set the formatting string template to the correct value.\n",
    "Once we actually connect the client, the client can then tell us the full link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.config[\"distributed\"][\"dashboard\"][\"link\"] = \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{host}:{port}/status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(scheduler_file=scheduler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link after `Dashboard:` above to get a visualization of the Dask Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the database\n",
    "When using dask the workers make the connections. To make a query we will pass in the url so each worker can make its own connection if it needs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_url = 'postgresql://desc_dc2_drp_user@nerscdb03.nersc.gov:5432/desc_dc2_drp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = \"run22i_dr6c_object\"\n",
    "table_name = \"dpdd_object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns we need.  This allows for significant performance advantages when reading a column-based storage format such as Parquet.\n",
    "columns_to_read = ['ra', 'dec', 'mag_g', 'mag_i', 'mag_r', 'magerr_g', 'magerr_i',\n",
    "                  'magerr_r', 'extendedness', 'tract']\n",
    "\n",
    "# Specify how to partition input.\n",
    "# Value of divisions should be, e.g., 17 tract numbers, starting with\n",
    "# first and ending with > last tract, to divide into 16 parallel queries.\n",
    "# The following definition encompasses all tracts\n",
    "div = [2723, 2905, 3080, 3258, 3268, 3449, 3635, 3825, 3834, 4028, 4226, 4235, 4436,\n",
    "      4639, 4648, 4858, 5075]\n",
    "\n",
    "# There is no problem reading in all tracts, but rendering plots is an issue.\n",
    "# For this notebook, we define a partition which only includes 16 tracts\n",
    "div_small = [3631,3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639,\n",
    "             3640, 3641, 3642, 3643, 3825, 3826, 3827, 3638]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_sql_table(table_name, pg_url, index_col=\"tract\", \n",
    "                       columns=columns_to_read,                                                                             \n",
    "                       schema='run22i_dr6c_object', divisions=div_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning is harmless. The special datatype (three floating point values) of the coord column isn't recognized, but it's not used for anything so it doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the data in Dask Cluster Worker memory\n",
    "\n",
    "Dask actively purges data from memory when its no longer needed by the Dask Task Graph currently doing the computation.\n",
    "\n",
    "That's not what we want here where we want to plot several quantities repeatedly.  So we explicitly tell Dask to persist this data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Create Color Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean.  \n",
    "snr_magerr_threshold = 0.3  # mag\n",
    "df = df[(df.magerr_g < snr_magerr_threshold) & (df.magerr_r < snr_magerr_threshold) & (df.magerr_i < snr_magerr_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['g-r'] = df['mag_g'] - df['mag_r']\n",
    "df['r-i'] = df['mag_r'] - df['mag_i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal = df[(df.extendedness > 0.95)]  \n",
    "star = df[(df.extendedness < 0.95)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HoloViews `Points` objects and Wrap with Datashader\n",
    "\n",
    "Create for position, magnitude and color.  Use datashader to provide rasterized images that display in finite time but are still zoomable.\n",
    "\n",
    "We will first define several different HoloViews objects\n",
    "Then we will use the `+` overloading to display the visualizations.\n",
    "\n",
    "For some more examples and information, see\n",
    "https://holoviews.org/user_guide/Large_Data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_ra_dec = hv.Points(df, kdims=['ra', 'dec'])\n",
    "ra_dec = datashade(points_ra_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_mag_magerr = hv.Points(df, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                         hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "mag_magerr = datashade(points_mag_magerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_color_mag = hv.Points(df, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "color_mag = datashade(points_color_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_color_color = hv.Points(df, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                          hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "color_color = datashade(points_color_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Holoviews one uses the `+` operator to put these three different visualizations next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_magerr + color_mag + color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color-magnitude and color-color plots will zoom together.  The synchronized range zooming is just based on matching the range of g-r. it's not subsetting the points in the view.  Thus there is no relationship between the RA, Dec plot and the other plots.\n",
    "\n",
    "Related to this, I don't know how to invert the mag_g axis in the middle plot without also inverting the mag_g axis in the left plot and without breaking the shared axes selections.  One can `color_mag.opts(invert_yaxis=True)`, but that would not just invert the `mag_magerr` plot axis, which would be annoying, but even worse it would invert the linked selection to be 28 < mag_g < 14, which is the empty set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashade Multiple Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how to separate by `extendedness` parameter.  Note that `extendedness` is pretty conservative with a high false negative at faint magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to force the range for magerr with 'range' instead of 'soft_range'.  I'm not sure why.\n",
    "gal_mag_magerr = hv.Points(gal, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                       hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "gal_color_mag = hv.Points(gal, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                      hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "gal_color_color = hv.Points(gal, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "\n",
    "star_mag_magerr = hv.Points(star, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                       hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "star_color_mag = hv.Points(star, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "star_color_color = hv.Points(star, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                          hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "\n",
    "typed_mag_magerr = {'gal': gal_mag_magerr, 'star': star_mag_magerr}\n",
    "typed_color_mag = {'gal': gal_color_mag, 'star': star_color_mag}\n",
    "typed_color_color = {'gal': gal_color_color, 'star': star_color_color}\n",
    "\n",
    "shaded_mag_magerr = datashade(hv.NdOverlay(typed_mag_magerr, kdims='type'),\n",
    "                              aggregator=ds.count_cat('type'))\n",
    "shaded_color_mag = datashade(hv.NdOverlay(typed_color_mag, kdims='type'),\n",
    "                             aggregator=ds.count_cat('type'))\n",
    "shaded_color_color = datashade(hv.NdOverlay(typed_color_color, kdims='type'),\n",
    "                               aggregator=ds.count_cat('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaded_mag_magerr + shaded_color_mag + shaded_color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The galaxies are red, while the \"stars\" (== not obviously extended) are blue.\n",
    "\n",
    "Zoom in to mag - magerr plot to see that the outlying cluster of higher uncertainties as a function of magnitude are galaxies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "1. Constructing a legend for the above is unfortunately a little unobvious and awkward.  We lost the information when we datashaded the NdOverlay.  You could do it by creating a new set of empty NdOverlay object to get the colors. \n",
    "\n",
    "This is relying on the fact that the above commands used the default color map.  \n",
    "Explicitly specifying the color above would have been better.\n",
    "```\n",
    "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
    "color_key = {k: Sets1to3[i] for i, k in enumerate(typed_color_mag)}\n",
    "color_points = hv.NdOverlay({k: hv.Points([gal['g-r'][0], gal['mag_g'][0]],\n",
    "                                          label=str(k)).options(color=v) for k, v in color_key.items()})\n",
    "                                          \n",
    "shaded_color_mag * color_points + shaded_color_color * color_points\n",
    "```\n",
    "\n",
    "Above code adapted from http://holoviews.org/user_guide/Large_Data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use hover-over aggregation\n",
    "\n",
    "We can set up a dynamic hover-over that gives information about the local area.  In this case we're just doing a count of the number of points in a given rectangular region.\n",
    "\n",
    "Note the use of the `*` to compose the results of `datashade` and `hv.util.Dynamic`.  This is the idiom in Holoviews to combine several different visualizations/tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.streams import RangeXY\n",
    "\n",
    "# A funnily-named wrapper function to generate hover-overs by count.\n",
    "# nx, ny are fixed by the original range.  We don't get finer resolution as we zoom in.\n",
    "def dynamate(points, width=400, height=400, nx=50, ny=50):\n",
    "    \"\"\"Datashades points at width, height.  Hover-over in dynamic boxes of nx x ny at given display size\"\"\"\n",
    "    datashaded_points = datashade(points, width=width, height=height)\n",
    "    hover_over_count = \\\n",
    "        hv.util.Dynamic(rasterize(points, width=nx, height=ny, streams=[RangeXY]),\n",
    "                        operation=hv.QuadMesh)\n",
    "    return datashaded_points * hover_over_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_mag_magerr = hv.Points(df, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                         hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "mag_magerr = datashade(points_mag_magerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
    "dynamic_mag_magerr = dynamate(points_mag_magerr)\n",
    "dynamic_color_mag = dynamate(points_color_mag)\n",
    "dynamic_color_color = dynamate(points_color_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_mag_magerr + dynamic_color_mag + dynamic_color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut down Dask Cluster\n",
    "\n",
    "When you're done, go back to your Terminal window and log out of the interactive node.  This will both shut down the Dask Cluster and log out of your interactive node job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python-bleed",
   "language": "python",
   "name": "desc-python-bleed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
