{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 2.2i Object Tables via PostgreSQL with Dask+Holoviews\n",
    "<br>Owner: **Joanne Bogart** ([@jrbogart](https://github.com/LSSTDESC/DC2-analysis/issues/new?body=@wmwv))\n",
    "<br>Last Verified to Run: \n",
    "    \n",
    "* Demonstrate the use of the DPDD-style Object table stored in PostgreSQL db. \n",
    "* Uses Dask+Holoviews+Datashader to demonstrate visualizing the entire dataset.\n",
    "\n",
    "Learning Objectives:\n",
    "After completing and studying this Notebook, a person should be able to\n",
    "1. Access database tables with Dask.\n",
    "    - Understand that specifying the columns to select increases performance.\n",
    "2. Make a plot using Holoviews\n",
    "3. Use Datashader to interactively rasterize a large dataset for display.\n",
    "4. Launch a set of Dask workers on a SLURM qos=interactive job.\n",
    "\n",
    "Logistics:\n",
    "\n",
    "1. These tests were conducted on NERSC through the https://jupyter.nersc.gov interface.\n",
    "\n",
    "2. Requires:\n",
    "```\n",
    "dask\n",
    "dask.distributed\n",
    "holoviews\n",
    "datashader\n",
    "bokeh\n",
    "psycopg2\n",
    "sqlalchemy\n",
    "```\n",
    "\n",
    "These were used with `desc-python-bleed` kernel\n",
    "\n",
    "3. This was run using the `desc-python-bleed` kernel\n",
    "\n",
    "Current Status:\n",
    "* Quick demonstration for people who want to check out use of PostgreSQL with DASK.\n",
    "* Future work planned: try matching Run2.2i dr6c with truth\n",
    "\n",
    "References:  \n",
    "    https://dask.org  \n",
    "    https://datashader.org   \n",
    "    https://holoviews.org  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import datashade, shade, dynspread, rasterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities to help with making a delayed query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DaskQuery:\n",
    "    def __init__(self, base_query, column_types, column_names):\n",
    "        '''\n",
    "        base_query is a string of the format \n",
    "        'select.. where..  (index_column >= {lower}) and (index_column < {upper})'\n",
    "        column_types is a dict {col_name1 : dtype1, col_name2 : dtype2...}\n",
    "        '''\n",
    "        self._base = base_query\n",
    "        self._column_types = column_types\n",
    "        self._column_names = column_names\n",
    "    @property\n",
    "    def column_types(self):\n",
    "        return self._column_types\n",
    "    \n",
    "    @property\n",
    "    def column_names(self):\n",
    "        return self._column_names\n",
    "    \n",
    "    def format(self, part):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        part   is a duple of limits on some indexed column\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        text string representation of an sql query\n",
    "        '''\n",
    "        lower = part[0]\n",
    "        upper = part[1]\n",
    "        return self._base.format(**locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_connection(url, echo=False):\n",
    "    engine = create_engine(url, echo=echo)\n",
    "    return engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def fetch_partition(base_query, part):\n",
    "    '''\n",
    "    Handles one part of the partition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_query  DaskQuery   template for query + type information\n",
    "    part        int tuple   limits on indexed column for this bit of the partition\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe\n",
    "    '''\n",
    "    conn = make_connection(pg_url)\n",
    "    result = conn.execute(base_query.format(part)).fetchall()\n",
    "    df = pd.DataFrame(result, columns=base_query.column_names)\n",
    "    # Explicitly set column types\n",
    "    return df.astype(base_query.column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parts(ix_list):\n",
    "    '''\n",
    "    ix_list   Monotone increasing list of index column values\n",
    "    returns:\n",
    "        list of duples, each defining an interval of index column values\n",
    "    '''\n",
    "    parts = []\n",
    "    for i in range(len(ix_list) - 1):\n",
    "        parts.append((ix_list[i], ix_list[i+1]))\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start our Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple testing and illustration of how to use dask, holoview, and datashader here you can run locally on just one tract.\n",
    "To run on the full set of DR6, you'll need to set up a node to support Dask distributed.  Basically you just need a machine that can hold the data in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a Dask Cluster on an Interactive Node\n",
    "\n",
    "Move to your SCRATCH directory to simplify file locking.  (On NERSC, the `SCRATCH` environment variable points to an individual SCRATCH area for each user.)\n",
    "```\n",
    "cd $SCRATCH/\n",
    "```\n",
    "\n",
    "In a separate Terminal on Cori (a JupyterHub Terminal works fine), ask for a Node from the `interactive` queue.  This generally completes in seconds.\n",
    "```\n",
    "salloc -N 1 -C haswell --qos=interactive -t 04:00:00\n",
    "```\n",
    "\n",
    "And then once on that Node, load the right Python environment:\n",
    "```\n",
    "python /global/common/software/lsst/common/miniconda/start-kernel-cli.py desc-python-bleed\n",
    "```\n",
    "\n",
    "And then start up the Dask Cluster\n",
    "```\n",
    "NUM_WORKERS=16\n",
    "SCHEDULER_FILE=${SCRATCH}/scheduler.json\n",
    "rm -rf ${SCHEDULER_FILE}\n",
    "dask-scheduler --scheduler-file ${SCHEDULER_FILE} &\n",
    "dask-worker --nprocs ${NUM_WORKERS} --scheduler-file ${SCHEDULER_FILE} &\n",
    "```\n",
    "\n",
    "(We explicitly `rm -rf ${SCHEDULER_FILE}` above in case it's still around from a previous invocation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You connect to this Dask cluster by passing in the location of the `SCHEDULER_FILE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_file = os.path.join(os.environ[\"SCRATCH\"], \"dask/scheduler.json\")  # modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure the dashboard URL to use the JupyterHub proxy service.\n",
    "We here set the formatting string template to the correct value.\n",
    "Once we actually connect the client, the client can then tell us the full link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.config[\"distributed\"][\"dashboard\"][\"link\"] = \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{host}:{port}/status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(scheduler_file=scheduler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link after `Dashboard:` above to get a visualization of the Dask Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the database\n",
    "When using dask we make might make a connection in the main line code in order to get metadata (column data types). In this notebook the types were known (all floats except for 'tract') so a suitable data structure is created below \"by hand\".\n",
    "\n",
    "When making a query we will pass in the url so each worker can make its own connection if it needs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_url = 'postgresql://desc_dc2_drp_user@nerscdb03.nersc.gov:5432/desc_dc2_drp' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_name = \"run22i_dr6c_object\"\n",
    "table_name = \"dpdd_object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Form our base query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns we need.  \n",
    "columns_to_read = ['ra', 'dec', 'mag_g', 'mag_i', 'mag_r', 'magerr_g', 'magerr_i',\n",
    "                   'magerr_r', 'extendedness', 'tract']\n",
    "column_types_dict = { col : float for col in columns_to_read}   # except for tract\n",
    "column_types_dict['tract'] = int\n",
    "column_list = ','.join(columns_to_read)\n",
    "\n",
    "# cut on good snr\n",
    "snr_magerr_threshold = 0.3\n",
    "clean = ' and '.join([f'(magerr_{band} < {snr_magerr_threshold})' for band in ['i', 'g', 'r']])\n",
    "\n",
    "q = f'''\n",
    "     select {column_list} from {schema_name}.{table_name} \n",
    "     where {clean}''' \n",
    "q += ' and (tract >= {lower}) and (tract < {upper})'\n",
    "d_query = DaskQuery(q, column_types_dict, columns_to_read)\n",
    "\n",
    "print(q)         # check the query template looks reasonable\n",
    "\n",
    "# value of divisions should be, e.g., 17 tract numbers, starting with\n",
    "# first and ending with last tract, to divide into 16 parallel queries\n",
    "# The following division would be suitable for all of run2.2i dr6c \n",
    "div = [2723, 2905, 3080, 3258, 3268, 3449, 3635, 3825, 3834, 4028, 4226, 4235, 4436,\n",
    "      4639, 4648, 4858, 5075]\n",
    "parts = make_parts(div)\n",
    "\n",
    "# Define a smaller partition (one tract per worker)\n",
    "div_small = [3631,3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639,\n",
    "             3640, 3641, 3642, 3643, 3825, 3826, 3827, 3638]\n",
    "parts_small = make_parts(div_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = dd.utils.make_meta(column_types_dict)\n",
    "\n",
    "#ddf = dd.from_delayed([fetch_partition(d_query, part) for part in parts_small],\n",
    "ddf = dd.from_delayed([fetch_partition(d_query, part) for part in parts],                      \n",
    "                     meta=m, divisions=div_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ddf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the data in Dask Cluster Worker memory\n",
    "\n",
    "Dask actively purges data from memory when its no longer needed by the Dask Task Graph currently doing the computation.\n",
    "\n",
    "That's not what we want here where we want to plot several quantities repeatedly.  So we explicitly tell Dask to persist this data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Color Columns, Define Populations\n",
    "The \"clean\" cut was already in the query so we don't have to do anything more here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['g-r'] = ddf['mag_g'] - ddf['mag_r']\n",
    "ddf['r-i'] = ddf['mag_r'] - ddf['mag_i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal = ddf[(ddf.extendedness > 0.95)]\n",
    "star = ddf[(ddf.extendedness < 0.95)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HoloViews `Points` objects and Wrap with Datashader\n",
    "\n",
    "Create for position, magnitude and color.  Use datashader to provide rasterized images that display in finite time but are still zoomable.\n",
    "\n",
    "We will first define several different HoloViews objects\n",
    "Then we will use the `+` overloading to display the visualizations.\n",
    "\n",
    "For some more examples and information, see\n",
    "https://holoviews.org/user_guide/Large_Data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_ra_dec = hv.Points(ddf, kdims=['ra', 'dec'])\n",
    "ra_dec = datashade(points_ra_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_mag_magerr = hv.Points(ddf, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                         hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "mag_magerr = datashade(points_mag_magerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_color_mag = hv.Points(ddf, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "color_mag = datashade(points_color_mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_color_color = hv.Points(ddf, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                          hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "color_color = datashade(points_color_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Holoviews one uses the `+` operator to put these three different visualizations next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_magerr + color_mag + color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color-magnitude and color-color plots will zoom together.  The synchronized range zooming is just based on matching the range of g-r. it's not subsetting the points in the view.  Thus there is no relationship between the RA, Dec plot and the other plots.\n",
    "\n",
    "Related to this, I don't know how to invert the mag_g axis in the middle plot without also inverting the mag_g axis in the left plot and without breaking the shared axes selections.  One can `color_mag.opts(invert_yaxis=True)`, but that would not just invert the `mag_magerr` plot axis, which would be annoying, but even worse it would invert the linked selection to be 28 < mag_g < 14, which is the empty set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashade Multiple Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how to separate by `extendedness` parameter.  Note that `extendedness` is pretty conservative with a high false negative at faint magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to force the range for magerr with 'range' instead of 'soft_range'.  I'm not sure why.\n",
    "gal_mag_magerr = hv.Points(gal, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                       hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "gal_color_mag = hv.Points(gal, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                      hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "gal_color_color = hv.Points(gal, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "\n",
    "star_mag_magerr = hv.Points(star, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                       hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "star_color_mag = hv.Points(star, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                        hv.Dimension('mag_g', soft_range=(14, 28))])\n",
    "star_color_color = hv.Points(star, kdims=[hv.Dimension('g-r', soft_range=(-2, 3)),\n",
    "                                          hv.Dimension('r-i', soft_range=(-2, 3))])\n",
    "\n",
    "typed_mag_magerr = {'gal': gal_mag_magerr, 'star': star_mag_magerr}\n",
    "typed_color_mag = {'gal': gal_color_mag, 'star': star_color_mag}\n",
    "typed_color_color = {'gal': gal_color_color, 'star': star_color_color}\n",
    "\n",
    "shaded_mag_magerr = datashade(hv.NdOverlay(typed_mag_magerr, kdims='type'),\n",
    "                              aggregator=ds.count_cat('type'))\n",
    "shaded_color_mag = datashade(hv.NdOverlay(typed_color_mag, kdims='type'),\n",
    "                             aggregator=ds.count_cat('type'))\n",
    "shaded_color_color = datashade(hv.NdOverlay(typed_color_color, kdims='type'),\n",
    "                               aggregator=ds.count_cat('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shaded_mag_magerr + shaded_color_mag + shaded_color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The galaxies are red, while the \"stars\" (== not obviously extended) are blue.\n",
    "\n",
    "Zoom in to mag - magerr plot to see that the outlying cluster of higher uncertainties as a function of magnitude are galaxies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "1. Constructing a legend for the above is unfortunately a little unobvious and awkward.  We lost the information when we datashaded the NdOverlay.  You could do it by creating a new set of empty NdOverlay object to get the colors. \n",
    "\n",
    "This is relying on the fact that the above commands used the default color map.  \n",
    "Explicitly specifying the color above would have been better.\n",
    "```\n",
    "from datashader.colors import Sets1to3 # default datashade() and shade() color cycle\n",
    "color_key = {k: Sets1to3[i] for i, k in enumerate(typed_color_mag)}\n",
    "color_points = hv.NdOverlay({k: hv.Points([gal['g-r'][0], gal['mag_g'][0]],\n",
    "                                          label=str(k)).options(color=v) for k, v in color_key.items()})\n",
    "                                          \n",
    "shaded_color_mag * color_points + shaded_color_color * color_points\n",
    "```\n",
    "\n",
    "Above code adapted from http://holoviews.org/user_guide/Large_Data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use hover-over aggregation\n",
    "\n",
    "We can set up a dynamic hover-over that gives information about the local area.  In this case we're just doing a count of the number of points in a given rectangular region.\n",
    "\n",
    "Note the use of the `*` to compose the results of `datashade` and `hv.util.Dynamic`.  This is the idiom in Holoviews to combine several different visualizations/tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holoviews.streams import RangeXY\n",
    "\n",
    "# A funnily-named wrapper function to generate hover-overs by count.\n",
    "# nx, ny are fixed by the original range.  We don't get finer resolution as we zoom in.\n",
    "def dynamate(points, width=400, height=400, nx=50, ny=50):\n",
    "    \"\"\"Datashades points at width, height.  Hover-over in dynamic boxes of nx x ny at given display size\"\"\"\n",
    "    datashaded_points = datashade(points, width=width, height=height)\n",
    "    hover_over_count = \\\n",
    "        hv.util.Dynamic(rasterize(points, width=nx, height=ny, streams=[RangeXY]),\n",
    "                        operation=hv.QuadMesh)\n",
    "    return datashaded_points * hover_over_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_mag_magerr = hv.Points(ddf, kdims=[hv.Dimension('mag_g', soft_range=(14, 28)),\n",
    "                                          hv.Dimension('magerr_g', range=(0, snr_magerr_threshold))])\n",
    "mag_magerr = datashade(points_mag_magerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts QuadMesh [tools=['hover']] (alpha=0 hover_alpha=0.2)\n",
    "dynamic_mag_magerr = dynamate(points_mag_magerr)\n",
    "dynamic_color_mag = dynamate(points_color_mag)\n",
    "dynamic_color_color = dynamate(points_color_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_mag_magerr + dynamic_color_mag + dynamic_color_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut down Dask Cluster\n",
    "\n",
    "When you're done, go back to your Terminal window and log out of the interactive node.  This will both shut down the Dask Cluster and log out of your interactive node job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "desc-python-bleed",
   "language": "python",
   "name": "desc-python-bleed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
